{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # For pre-processing\n",
    "import pandas as pd# For data handling\n",
    "from time import time # To time operations\n",
    "from collections import defaultdict # For word frequency\n",
    "import spacy # For pre-processing\n",
    "import logging  # To monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Martin Prince</td>\n",
       "      <td>I don't think there's anything left to say.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Bart?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>Victory party under the slide!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Mr. Bergstrom! Mr. Bergstrom!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ...\n",
       "5            Martin Prince        I don't think there's anything left to say.\n",
       "6  Edna Krabappel-Flanders                                              Bart?\n",
       "7             Bart Simpson                     Victory party under the slide!\n",
       "8                      NaN                                                NaN\n",
       "9             Lisa Simpson                      Mr. Bergstrom! Mr. Bergstrom!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between reset_index() and reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Martin Prince</td>\n",
       "      <td>I don't think there's anything left to say.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Bart?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>Victory party under the slide!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Mr. Bergstrom! Mr. Bergstrom!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Landlady</td>\n",
       "      <td>Hey, hey, he Moved out this morning. He must h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       raw_character_text  \\\n",
       "0      0              Miss Hoover   \n",
       "1      1             Lisa Simpson   \n",
       "2      2              Miss Hoover   \n",
       "3      3             Lisa Simpson   \n",
       "4      4  Edna Krabappel-Flanders   \n",
       "5      5            Martin Prince   \n",
       "6      6  Edna Krabappel-Flanders   \n",
       "7      7             Bart Simpson   \n",
       "8      9             Lisa Simpson   \n",
       "9     10                 Landlady   \n",
       "\n",
       "                                        spoken_words  \n",
       "0  No, actually, it was a little of both. Sometim...  \n",
       "1                             Where's Mr. Bergstrom?  \n",
       "2  I don't know. Although I'd sure like to talk t...  \n",
       "3                         That life is worth living.  \n",
       "4  The polls will be open from now until the end ...  \n",
       "5        I don't think there's anything left to say.  \n",
       "6                                              Bart?  \n",
       "7                     Victory party under the slide!  \n",
       "8                      Mr. Bergstrom! Mr. Bergstrom!  \n",
       "9  Hey, hey, he Moved out this morning. He must h...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna().reset_index()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Martin Prince</td>\n",
       "      <td>I don't think there's anything left to say.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Bart?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>Victory party under the slide!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Mr. Bergstrom! Mr. Bergstrom!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Landlady</td>\n",
       "      <td>Hey, hey, he Moved out this morning. He must h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ...\n",
       "5            Martin Prince        I don't think there's anything left to say.\n",
       "6  Edna Krabappel-Flanders                                              Bart?\n",
       "7             Bart Simpson                     Victory party under the slide!\n",
       "8             Lisa Simpson                      Mr. Bergstrom! Mr. Bergstrom!\n",
       "9                 Landlady  Hey, hey, he Moved out this morning. He must h..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('simpsons_dataset.csv')\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    0\n",
       "spoken_words          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = spacy.load('en',disable=['ner','parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    text = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    if len(text) > 2:\n",
    "        return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 1.41 mins\n"
     ]
    }
   ],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])\n",
    "\n",
    "t = time()\n",
    "\n",
    "text = [cleaning(doc) for doc in nl.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actually little disease magazine news show be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>be mr bergstrom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not know would sure like talk not touch lesson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>life worth live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>poll open end recess case decide thought will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not think be leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>victory party slide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mr bergstrom mr bergstrom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hey hey move morning new job take copernicus c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean\n",
       "0  actually little disease magazine news show be ...\n",
       "1                                    be mr bergstrom\n",
       "2  not know would sure like talk not touch lesson...\n",
       "3                                    life worth live\n",
       "4  poll open end recess case decide thought will ...\n",
       "5                                 not think be leave\n",
       "6                                               None\n",
       "7                                victory party slide\n",
       "8                          mr bergstrom mr bergstrom\n",
       "9  hey hey move morning new job take copernicus c..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean':text})\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean    38133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92412, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main reason to use bigrams is to catch words like \"mr_burns\" or \"bart_simpson\" !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:55:53: 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# As Phrases() takes a list of list of words as input:\n",
    "\n",
    "sent = [row.split() for row in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'little',\n",
       " 'disease',\n",
       " 'magazine',\n",
       " 'news',\n",
       " 'show',\n",
       " 'be',\n",
       " 'natural',\n",
       " 'think']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:01:07: collecting all words and their counts\n",
      "INFO - 21:01:07: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 21:01:07: PROGRESS: at sentence #10000, processed 67396 words and 50551 word types\n",
      "INFO - 21:01:07: PROGRESS: at sentence #20000, processed 140465 words and 95808 word types\n",
      "INFO - 21:01:07: PROGRESS: at sentence #30000, processed 207950 words and 132011 word types\n",
      "INFO - 21:01:08: PROGRESS: at sentence #40000, processed 270207 words and 164407 word types\n",
      "INFO - 21:01:08: PROGRESS: at sentence #50000, processed 334085 words and 196195 word types\n",
      "INFO - 21:01:08: PROGRESS: at sentence #60000, processed 400877 words and 228659 word types\n",
      "INFO - 21:01:08: PROGRESS: at sentence #70000, processed 467802 words and 260712 word types\n",
      "INFO - 21:01:08: PROGRESS: at sentence #80000, processed 534361 words and 292095 word types\n",
      "INFO - 21:01:08: PROGRESS: at sentence #90000, processed 602037 words and 321944 word types\n",
      "INFO - 21:01:08: collected 328658 word types from a corpus of 618920 words (unigram + bigrams) and 92412 sentences\n",
      "INFO - 21:01:08: using 328658 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:05:12: source_vocab length 328658\n",
      "INFO - 21:05:15: Phraser built with 127 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the corpus based on the bigrams detected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'little',\n",
       " 'disease',\n",
       " 'magazine',\n",
       " 'news',\n",
       " 'show',\n",
       " 'be',\n",
       " 'natural',\n",
       " 'think']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29673"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'actually': 432,\n",
       "             'little': 2130,\n",
       "             'disease': 46,\n",
       "             'magazine': 125,\n",
       "             'news': 252,\n",
       "             'show': 213,\n",
       "             'be': 35473,\n",
       "             'natural': 78,\n",
       "             'think': 3765,\n",
       "             'mr': 808,\n",
       "             'bergstrom': 18,\n",
       "             'not': 14093,\n",
       "             'know': 4998,\n",
       "             'would': 1208,\n",
       "             'sure': 1250,\n",
       "             'like': 5322,\n",
       "             'talk': 987,\n",
       "             'touch': 197,\n",
       "             'lesson': 165,\n",
       "             'plan': 309,\n",
       "             'teach': 329,\n",
       "             'life': 1244,\n",
       "             'worth': 144,\n",
       "             'live': 762,\n",
       "             'poll': 20,\n",
       "             'open': 432,\n",
       "             'end': 469,\n",
       "             'recess': 11,\n",
       "             'case': 220,\n",
       "             'decide': 136,\n",
       "             'thought': 119,\n",
       "             'will': 6281,\n",
       "             'final': 108,\n",
       "             'statement': 21,\n",
       "             'martin': 121,\n",
       "             'leave': 1061,\n",
       "             'victory': 31,\n",
       "             'party': 428,\n",
       "             'slide': 48,\n",
       "             'hey': 3753,\n",
       "             'move': 170,\n",
       "             'morning': 309,\n",
       "             'new': 1336,\n",
       "             'job': 599,\n",
       "             'take': 914,\n",
       "             'copernicus': 5,\n",
       "             'costume': 65,\n",
       "             'train': 136,\n",
       "             'capital_city': 40,\n",
       "             'traditional': 19,\n",
       "             'environmentally': 3,\n",
       "             'sound': 301,\n",
       "             'yes': 1158,\n",
       "             'backbone': 7,\n",
       "             'country': 248,\n",
       "             'leland': 1,\n",
       "             'stanford': 5,\n",
       "             'drive': 439,\n",
       "             'golden': 53,\n",
       "             'spike': 15,\n",
       "             'promontory': 1,\n",
       "             'point': 387,\n",
       "             'thank': 1334,\n",
       "             'vote': 168,\n",
       "             'man': 2721,\n",
       "             'voting': 9,\n",
       "             \"'s\": 5210,\n",
       "             'geek': 21,\n",
       "             'get': 3130,\n",
       "             'right': 3576,\n",
       "             'girl': 699,\n",
       "             'sweat': 35,\n",
       "             'long': 710,\n",
       "             'couple': 206,\n",
       "             'people': 1536,\n",
       "             'milhouse': 523,\n",
       "             'bart': 2801,\n",
       "             'recount': 3,\n",
       "             'want': 3250,\n",
       "             'way': 1725,\n",
       "             'mister': 117,\n",
       "             'president': 265,\n",
       "             'board': 89,\n",
       "             'track': 78,\n",
       "             'afternoon': 87,\n",
       "             'delight': 20,\n",
       "             'come': 3236,\n",
       "             'shelbyville': 86,\n",
       "             'parkville': 1,\n",
       "             'oh': 6646,\n",
       "             'mean': 1304,\n",
       "             'go': 2846,\n",
       "             'ah': 854,\n",
       "             'sorry': 1306,\n",
       "             'lisa': 1786,\n",
       "             'substitute': 24,\n",
       "             'teacher': 232,\n",
       "             'fraud': 32,\n",
       "             'today': 749,\n",
       "             'wear': 404,\n",
       "             'gym': 55,\n",
       "             'short': 164,\n",
       "             'tomorrow': 366,\n",
       "             'speak': 239,\n",
       "             'french': 90,\n",
       "             'pretend': 97,\n",
       "             'run': 591,\n",
       "             'band': 152,\n",
       "             'saw': 13,\n",
       "             'god': 643,\n",
       "             'good': 3748,\n",
       "             'true': 352,\n",
       "             'lie': 387,\n",
       "             'need': 1850,\n",
       "             'project': 92,\n",
       "             'problem': 467,\n",
       "             'middle': 97,\n",
       "             'class': 331,\n",
       "             'anybody': 134,\n",
       "             'care': 563,\n",
       "             'abandon': 48,\n",
       "             'understand': 307,\n",
       "             'miss': 619,\n",
       "             'feel': 1072,\n",
       "             'rely': 8,\n",
       "             'guess': 757,\n",
       "             'mind': 407,\n",
       "             'alongside': 4,\n",
       "             'speed': 64,\n",
       "             'goodbye': 154,\n",
       "             'honey': 539,\n",
       "             'okay': 1957,\n",
       "             'read': 483,\n",
       "             'note': 102,\n",
       "             'throw': 369,\n",
       "             'big': 1168,\n",
       "             'bash': 22,\n",
       "             'champagne': 33,\n",
       "             'musician': 24,\n",
       "             'holy': 89,\n",
       "             'bad': 1088,\n",
       "             'thing': 2117,\n",
       "             'happen': 694,\n",
       "             'alright': 54,\n",
       "             'allright': 2,\n",
       "             'spill': 37,\n",
       "             'milk': 128,\n",
       "             'mopey': 1,\n",
       "             'tell': 1958,\n",
       "             'father': 678,\n",
       "             'forever': 229,\n",
       "             'glad': 235,\n",
       "             'cry': 164,\n",
       "             'hate': 360,\n",
       "             'base': 109,\n",
       "             'emotion': 17,\n",
       "             'sir': 943,\n",
       "             'baboon': 16,\n",
       "             'realize': 185,\n",
       "             'say': 1645,\n",
       "             'whoa': 313,\n",
       "             'somebody': 202,\n",
       "             'bind': 31,\n",
       "             'day': 1795,\n",
       "             'believe': 760,\n",
       "             'hear': 1023,\n",
       "             'marge': 2385,\n",
       "             'call': 603,\n",
       "             'stupid': 607,\n",
       "             'ugliest': 3,\n",
       "             'smelly': 11,\n",
       "             'ape': 54,\n",
       "             'homer': 3059,\n",
       "             'allow': 128,\n",
       "             'hurt': 333,\n",
       "             'feeling': 193,\n",
       "             'little_girl': 146,\n",
       "             'upstairs': 34,\n",
       "             'confidence': 26,\n",
       "             'shake': 109,\n",
       "             'happy': 613,\n",
       "             'faith': 73,\n",
       "             'daddy': 345,\n",
       "             'hold': 468,\n",
       "             'look': 3468,\n",
       "             'forgive': 120,\n",
       "             'maybe': 1064,\n",
       "             'help': 1096,\n",
       "             'lose': 629,\n",
       "             'special': 342,\n",
       "             'lucky': 162,\n",
       "             'roof': 63,\n",
       "             'child': 808,\n",
       "             'time': 2635,\n",
       "             'bed': 285,\n",
       "             'lot': 826,\n",
       "             'probably': 213,\n",
       "             'place': 823,\n",
       "             'food': 383,\n",
       "             'real': 676,\n",
       "             'guy': 1616,\n",
       "             'serve': 117,\n",
       "             'drink': 422,\n",
       "             'explain': 118,\n",
       "             'fix': 151,\n",
       "             'doll': 91,\n",
       "             'house': 835,\n",
       "             'monkey': 217,\n",
       "             'work': 1424,\n",
       "             'nail': 59,\n",
       "             'tail': 46,\n",
       "             'dad': 2055,\n",
       "             'matter': 257,\n",
       "             'son': 971,\n",
       "             'lewi': 21,\n",
       "             'money': 966,\n",
       "             'neat': 22,\n",
       "             'ball': 284,\n",
       "             'world': 802,\n",
       "             'series': 69,\n",
       "             'huh': 576,\n",
       "             'let': 1367,\n",
       "             'baby': 915,\n",
       "             'bottle': 132,\n",
       "             'motto': 8,\n",
       "             'moly': 10,\n",
       "             'parenting': 17,\n",
       "             'sleep': 391,\n",
       "             'maggie': 497,\n",
       "             'let_-PRON-': 1680,\n",
       "             'roll': 214,\n",
       "             'mmm': 148,\n",
       "             'hor': 8,\n",
       "             'doover': 1,\n",
       "             'promise': 276,\n",
       "             'eat': 983,\n",
       "             'have': 3208,\n",
       "             'go_to': 2543,\n",
       "             'pay': 631,\n",
       "             'friend': 871,\n",
       "             'invite': 104,\n",
       "             'home': 1058,\n",
       "             'mom': 1158,\n",
       "             'witty': 13,\n",
       "             'banter': 2,\n",
       "             'sophisticated': 18,\n",
       "             'adult': 121,\n",
       "             'yeah': 2380,\n",
       "             'fun': 590,\n",
       "             'old': 1263,\n",
       "             'better': 340,\n",
       "             'hmmm': 181,\n",
       "             'mmmm': 58,\n",
       "             'gag': 31,\n",
       "             'ice': 106,\n",
       "             'cub': 9,\n",
       "             'record': 163,\n",
       "             'bartender': 36,\n",
       "             'ph': 6,\n",
       "             'd': 297,\n",
       "             'mixology': 1,\n",
       "             'try': 1043,\n",
       "             'flander': 393,\n",
       "             'planter': 1,\n",
       "             'punch': 121,\n",
       "             'alcohol': 65,\n",
       "             'au': 14,\n",
       "             'contraire': 3,\n",
       "             'simpson': 1004,\n",
       "             'shot': 121,\n",
       "             'rum': 14,\n",
       "             'jigger': 3,\n",
       "             'bourbon': 11,\n",
       "             'dab': 6,\n",
       "             'roo': 3,\n",
       "             'creme': 6,\n",
       "             'de': 160,\n",
       "             'cassis': 1,\n",
       "             'flavor': 46,\n",
       "             'warm': 90,\n",
       "             'sense': 116,\n",
       "             'ssslurr': 1,\n",
       "             'shpeech': 1,\n",
       "             'easy': 345,\n",
       "             'al': 48,\n",
       "             'ky': 2,\n",
       "             'hol': 2,\n",
       "             'remember': 711,\n",
       "             'year': 1003,\n",
       "             'winfield': 6,\n",
       "             'laundry': 36,\n",
       "             'hamper': 7,\n",
       "             'hi': 384,\n",
       "             'sister': 286,\n",
       "             'law': 210,\n",
       "             'beau': 4,\n",
       "             'tiful': 3,\n",
       "             'ow': 199,\n",
       "             'jeez': 32,\n",
       "             'kind': 565,\n",
       "             'mace': 7,\n",
       "             'painful': 20,\n",
       "             'dr_hibbert': 46,\n",
       "             'enjoy': 321,\n",
       "             'uh': 2429,\n",
       "             'slip': 68,\n",
       "             'novelty': 26,\n",
       "             'cube': 41,\n",
       "             'fake': 118,\n",
       "             'fly': 263,\n",
       "             'highly': 51,\n",
       "             'toxic': 18,\n",
       "             'chemical': 23,\n",
       "             'ironically': 11,\n",
       "             'sanitary': 4,\n",
       "             'face': 524,\n",
       "             'priceless': 22,\n",
       "             'cute': 139,\n",
       "             'everybody': 443,\n",
       "             'funniest': 16,\n",
       "             'king': 215,\n",
       "             'wantin': 5,\n",
       "             \"'\": 2850,\n",
       "             'nerve': 20,\n",
       "             'wife': 535,\n",
       "             'meet': 488,\n",
       "             'hour': 433,\n",
       "             'ago': 129,\n",
       "             'stink': 121,\n",
       "             'lousy': 133,\n",
       "             'operation': 59,\n",
       "             'quit': 225,\n",
       "             'gee': 127,\n",
       "             'handful': 11,\n",
       "             'peanut': 72,\n",
       "             'maude': 80,\n",
       "             'invitin': 2,\n",
       "             'wonderful': 228,\n",
       "             'night': 822,\n",
       "             'suggest': 51,\n",
       "             'kid': 1955,\n",
       "             'young': 317,\n",
       "             'hat': 296,\n",
       "             'parent': 259,\n",
       "             'fight': 377,\n",
       "             'car': 708,\n",
       "             'music': 266,\n",
       "             'send': 369,\n",
       "             'chill': 28,\n",
       "             'spine': 22,\n",
       "             'act': 253,\n",
       "             'wet': 81,\n",
       "             'clothe': 119,\n",
       "             'dry': 87,\n",
       "             'martini': 10,\n",
       "             'lord': 277,\n",
       "             'glass': 164,\n",
       "             'pronounce': 29,\n",
       "             'whimsical': 3,\n",
       "             'jape': 3,\n",
       "             'season': 93,\n",
       "             'patient': 30,\n",
       "             'tolerant': 5,\n",
       "             'woman': 601,\n",
       "             'line': 297,\n",
       "             'cross': 94,\n",
       "             'stop': 1107,\n",
       "             'love': 1922,\n",
       "             'forget': 551,\n",
       "             'church': 214,\n",
       "             'stay': 533,\n",
       "             'scar': 60,\n",
       "             'inside': 229,\n",
       "             'notice': 118,\n",
       "             'strange': 61,\n",
       "             'admit': 116,\n",
       "             'hope': 470,\n",
       "             'respect': 132,\n",
       "             'sneak': 60,\n",
       "             'preview': 10,\n",
       "             'week': 476,\n",
       "             'sermon': 23,\n",
       "             'announcement': 43,\n",
       "             'pamphlet': 19,\n",
       "             'available': 35,\n",
       "             'newsrack': 1,\n",
       "             'include': 80,\n",
       "             'bible': 98,\n",
       "             'baffler': 1,\n",
       "             'satan': 47,\n",
       "             'boner': 11,\n",
       "             'grief': 13,\n",
       "             'teen': 44,\n",
       "             'cool': 498,\n",
       "             'fry': 91,\n",
       "             'hell': 430,\n",
       "             'compete': 24,\n",
       "             'squeaking': 1,\n",
       "             'homer_simpson': 429,\n",
       "             'shoe': 171,\n",
       "             'seat': 170,\n",
       "             'mrs': 149,\n",
       "             'lovejoy': 37,\n",
       "             'annual': 40,\n",
       "             'marriage': 226,\n",
       "             'retreat': 12,\n",
       "             'weekend': 98,\n",
       "             'catfish': 10,\n",
       "             'lake': 44,\n",
       "             'psychological': 9,\n",
       "             'counseling': 10,\n",
       "             'hang': 226,\n",
       "             'thread': 12,\n",
       "             'tune': 62,\n",
       "             'wish': 420,\n",
       "             'participate': 16,\n",
       "             'sign': 317,\n",
       "             'service': 131,\n",
       "             'attend': 29,\n",
       "             'tempting': 8,\n",
       "             'idea': 438,\n",
       "             'encounter': 10,\n",
       "             'fishing': 15,\n",
       "             'hello': 631,\n",
       "             'mrs_simpson': 117,\n",
       "             'suppose': 359,\n",
       "             'sitter': 24,\n",
       "             'dear': 308,\n",
       "             'find': 1287,\n",
       "             'babysitter': 28,\n",
       "             'kick': 196,\n",
       "             'tooth': 173,\n",
       "             'half': 290,\n",
       "             'dare': 93,\n",
       "             'tone': 27,\n",
       "             'young_lady': 39,\n",
       "             'taste': 163,\n",
       "             'hand': 582,\n",
       "             'wonder': 261,\n",
       "             'babysit': 8,\n",
       "             'ask': 588,\n",
       "             'desperate': 33,\n",
       "             'resort': 21,\n",
       "             'grampa': 290,\n",
       "             'feeb': 2,\n",
       "             'count': 143,\n",
       "             'dagnabit': 4,\n",
       "             'agin': 2,\n",
       "             'puttin': 17,\n",
       "             'trunk': 29,\n",
       "             'fever': 38,\n",
       "             'number': 386,\n",
       "             'stick': 327,\n",
       "             'finger': 126,\n",
       "             'electrical': 14,\n",
       "             'socket': 4,\n",
       "             'pine': 22,\n",
       "             'cleanser': 5,\n",
       "             'behave': 27,\n",
       "             'fall': 292,\n",
       "             'bathtub': 17,\n",
       "             'hurry': 120,\n",
       "             'list': 114,\n",
       "             'uh_huh': 125,\n",
       "             'smoke': 139,\n",
       "             'cigar': 24,\n",
       "             'gas': 156,\n",
       "             'fill': 216,\n",
       "             'er': 86,\n",
       "             'stretch': 33,\n",
       "             'leg': 171,\n",
       "             'general': 52,\n",
       "             'sherman': 20,\n",
       "             'ya': 897,\n",
       "             'wait_wait': 99,\n",
       "             'minute': 453,\n",
       "             'wait_minute': 322,\n",
       "             'part': 40,\n",
       "             'weigh': 29,\n",
       "             'upwards': 4,\n",
       "             'pound': 112,\n",
       "             'picture': 283,\n",
       "             'exactly': 183,\n",
       "             'freakishly': 2,\n",
       "             'hmmmm': 11,\n",
       "             'gentleman': 153,\n",
       "             'catch': 306,\n",
       "             'supermarket': 16,\n",
       "             'video': 125,\n",
       "             'store': 319,\n",
       "             'grab': 90,\n",
       "             'krusty': 580,\n",
       "             'burger': 55,\n",
       "             'head': 653,\n",
       "             'arcade': 8,\n",
       "             'kindly': 20,\n",
       "             'trust': 150,\n",
       "             'advantage': 35,\n",
       "             'lis': 185,\n",
       "             'crazy': 395,\n",
       "             'topsy': 3,\n",
       "             'turvy': 3,\n",
       "             'wrong': 538,\n",
       "             'gut': 63,\n",
       "             'bleed': 41,\n",
       "             'gramp': 3,\n",
       "             'welcome': 448,\n",
       "             'reverend': 67,\n",
       "             'helen': 54,\n",
       "             'hel': 26,\n",
       "             'lo': 38,\n",
       "             'spit': 65,\n",
       "             'shine': 43,\n",
       "             'chance': 237,\n",
       "             'afraid': 386,\n",
       "             'reconcile': 2,\n",
       "             'bait': 16,\n",
       "             'hook': 70,\n",
       "             'honesty': 14,\n",
       "             'away': 689,\n",
       "             'bowl': 109,\n",
       "             'expression': 29,\n",
       "             'turnout': 4,\n",
       "             'room': 493,\n",
       "             'introduce': 57,\n",
       "             'little_bit': 57,\n",
       "             'john': 110,\n",
       "             'gloria': 14,\n",
       "             'johnny': 53,\n",
       "             'boy': 1909,\n",
       "             'able': 113,\n",
       "             'cut': 417,\n",
       "             'manwise': 1,\n",
       "             'odor': 15,\n",
       "             'gin': 12,\n",
       "             'sour': 23,\n",
       "             'defeat': 36,\n",
       "             'press': 81,\n",
       "             'cook': 83,\n",
       "             'keep': 193,\n",
       "             'filthy': 40,\n",
       "             'profanely': 1,\n",
       "             'queen': 116,\n",
       "             'harpy': 1,\n",
       "             'crown': 17,\n",
       "             'majesty': 28,\n",
       "             'eye': 583,\n",
       "             'beautiful': 363,\n",
       "             'save': 575,\n",
       "             'bring': 690,\n",
       "             'happiness': 51,\n",
       "             'pass': 272,\n",
       "             'collection': 53,\n",
       "             'plate': 69,\n",
       "             'ned': 169,\n",
       "             'god_bless': 35,\n",
       "             'underline': 3,\n",
       "             'passage': 10,\n",
       "             'gun': 214,\n",
       "             'ohhh': 83,\n",
       "             'drunk': 161,\n",
       "             'dress': 159,\n",
       "             'fault': 153,\n",
       "             'interrupt': 38,\n",
       "             'turn': 851,\n",
       "             'self': 125,\n",
       "             'center': 126,\n",
       "             'birthday': 189,\n",
       "             'anniversary': 51,\n",
       "             'holiday': 63,\n",
       "             'religious': 30,\n",
       "             'secular': 3,\n",
       "             'chew': 76,\n",
       "             'mouth': 201,\n",
       "             'gamble': 40,\n",
       "             'seedy': 2,\n",
       "             'bar': 338,\n",
       "             'bum': 63,\n",
       "             'lowlife': 4,\n",
       "             'blow': 257,\n",
       "             'nose': 145,\n",
       "             'towel': 48,\n",
       "             'put': 114,\n",
       "             'gallon': 17,\n",
       "             'chocolate': 114,\n",
       "             'brownie': 23,\n",
       "             'fudge': 32,\n",
       "             'chip': 83,\n",
       "             'write': 476,\n",
       "             'shopping': 45,\n",
       "             'aisle': 21,\n",
       "             'step': 188,\n",
       "             'carton': 13,\n",
       "             'change': 492,\n",
       "             'make': 727,\n",
       "             'noise': 66,\n",
       "             'wake': 184,\n",
       "             'honking': 2,\n",
       "             'scratch': 47,\n",
       "             'key': 173,\n",
       "             'wait': 799,\n",
       "             'toenail': 6,\n",
       "             'yellow': 73,\n",
       "             'tired': 146,\n",
       "             'chest': 39,\n",
       "             'luau': 3,\n",
       "             'mcbain': 35,\n",
       "             'cannon': 21,\n",
       "             'regulation': 9,\n",
       "             'department': 61,\n",
       "             'book': 536,\n",
       "             \"gettin_'\": 145,\n",
       "             'pretty': 473,\n",
       "             'late': 302,\n",
       "             'use': 653,\n",
       "             'ethical': 5,\n",
       "             'crisis': 36,\n",
       "             'thirty': 232,\n",
       "             'clean': 300,\n",
       "             'seven': 286,\n",
       "             'incriminate': 5,\n",
       "             'evidence': 42,\n",
       "             'perfect': 303,\n",
       "             'crime': 128,\n",
       "             'fish': 184,\n",
       "             'selfishness': 2,\n",
       "             'anytime': 18,\n",
       "             'honest': 99,\n",
       "             'walk': 344,\n",
       "             'get_to': 655,\n",
       "             'husband': 314,\n",
       "             'forgot': 16,\n",
       "             'ahead': 154,\n",
       "             'waste': 165,\n",
       "             'strength': 38,\n",
       "             'ugly': 113,\n",
       "             'skillet': 1,\n",
       "             'butter': 68,\n",
       "             'ma': 100,\n",
       "             'coffee': 132,\n",
       "             'blowout': 2,\n",
       "             'casa': 4,\n",
       "             'frail': 3,\n",
       "             'joint': 24,\n",
       "             'twoish': 1,\n",
       "             'square': 94,\n",
       "             'have_get': 1443,\n",
       "             'funky': 15,\n",
       "             'noon': 21,\n",
       "             'exercise': 51,\n",
       "             'backwards': 36,\n",
       "             'spouse': 6,\n",
       "             'recommend': 28,\n",
       "             'counselor': 21,\n",
       "             'instance': 7,\n",
       "             'partner': 81,\n",
       "             'percent': 100,\n",
       "             'willing': 38,\n",
       "             'certificate': 16,\n",
       "             'frame': 53,\n",
       "             'word': 551,\n",
       "             'yanks': 5,\n",
       "             \"comin_'\": 170,\n",
       "             'famous': 107,\n",
       "             'fisherman': 1,\n",
       "             'bald': 58,\n",
       "             'cable': 61,\n",
       "             'mackerel': 1,\n",
       "             'pal': 114,\n",
       "             'cherry': 29,\n",
       "             'chick': 82,\n",
       "             'obvious': 32,\n",
       "             'degrade': 2,\n",
       "             'set': 307,\n",
       "             'movement': 14,\n",
       "             'decade': 18,\n",
       "             'great': 1422,\n",
       "             'shut': 312,\n",
       "             'door': 288,\n",
       "             'haw_haw': 75,\n",
       "             'haw': 24,\n",
       "             'hellion': 4,\n",
       "             'belt': 68,\n",
       "             'nice': 689,\n",
       "             'tie': 146,\n",
       "             'nelson': 195,\n",
       "             'thanks': 35,\n",
       "             'fail': 123,\n",
       "             'useless': 30,\n",
       "             'strong': 117,\n",
       "             'unpleasant': 14,\n",
       "             'remorse': 8,\n",
       "             'vile': 3,\n",
       "             'burlesque': 12,\n",
       "             'irrepressible': 3,\n",
       "             'youth': 38,\n",
       "             'bucket': 51,\n",
       "             'brush': 44,\n",
       "             'harder': 28,\n",
       "             'faster': 52,\n",
       "             'champion': 42,\n",
       "             'loser': 145,\n",
       "             'because': 547,\n",
       "             'worlllld': 1,\n",
       "             'trouble': 240,\n",
       "             'expect': 120,\n",
       "             'represent': 47,\n",
       "             'hero': 177,\n",
       "             'weirdo': 24,\n",
       "             'worm': 30,\n",
       "             'selfish': 34,\n",
       "             'wow': 657,\n",
       "             'give': 663,\n",
       "             'fame': 23,\n",
       "             'breakfast': 117,\n",
       "             'toss': 50,\n",
       "             'secret': 276,\n",
       "             'fawcet': 1,\n",
       "             'boo': 72,\n",
       "             'hoo': 67,\n",
       "             'sad': 164,\n",
       "             'person': 201,\n",
       "             'fool': 165,\n",
       "             'sucker': 68,\n",
       "             'hee_hee': 37,\n",
       "             'hee': 21,\n",
       "             'yup': 20,\n",
       "             'dwell': 6,\n",
       "             'fury': 5,\n",
       "             'fella': 92,\n",
       "             'close': 348,\n",
       "             'foot': 248,\n",
       "             'tall': 56,\n",
       "             'arm': 153,\n",
       "             'tree': 226,\n",
       "             'steel': 28,\n",
       "             'cold': 156,\n",
       "             'hard': 450,\n",
       "             'shock': 49,\n",
       "             'hair': 343,\n",
       "             'red': 204,\n",
       "             'fire': 453,\n",
       "             'convention': 29,\n",
       "             'soon': 239,\n",
       "             'comic': 66,\n",
       "             'fallout': 23,\n",
       "             'ward': 7,\n",
       "             'buy': 694,\n",
       "             'casper': 5,\n",
       "             'wimpy': 1,\n",
       "             'ghost': 69,\n",
       "             'equate': 1,\n",
       "             'friendliness': 1,\n",
       "             'wimpiness': 2,\n",
       "             'achieve': 25,\n",
       "             'popularity': 13,\n",
       "             'richie': 9,\n",
       "             'rich': 204,\n",
       "             'alike': 16,\n",
       "             'die': 574,\n",
       "             'hollow': 19,\n",
       "             'pursuit': 10,\n",
       "             'lighten': 22,\n",
       "             'radioactive_man': 53,\n",
       "             'rule': 176,\n",
       "             'wittier': 1,\n",
       "             'superhero': 15,\n",
       "             'knock': 150,\n",
       "             'sun': 96,\n",
       "             'hot': 328,\n",
       "             'dressed': 9,\n",
       "             'popular': 102,\n",
       "             'cartoon': 139,\n",
       "             'character': 126,\n",
       "             'discount': 24,\n",
       "             'ahem': 5,\n",
       "             'springfield': 986,\n",
       "             'mayor': 143,\n",
       "             'would_like': 476,\n",
       "             'funny': 330,\n",
       "             'pump': 50,\n",
       "             'dollar': 433,\n",
       "             'local': 132,\n",
       "             'economy': 26,\n",
       "             'youthful': 11,\n",
       "             'high': 261,\n",
       "             'spirit': 118,\n",
       "             'impart': 5,\n",
       "             'glow': 20,\n",
       "             'war': 205,\n",
       "             'horse': 156,\n",
       "             'radiation': 22,\n",
       "             'jerk': 173,\n",
       "             'stand': 352,\n",
       "             'correct': 50,\n",
       "             'clear': 150,\n",
       "             'shriner': 2,\n",
       "             'punk': 47,\n",
       "             'diamond': 50,\n",
       "             'joe': 122,\n",
       "             'quimby': 78,\n",
       "             'excuse': 312,\n",
       "             'left': 121,\n",
       "             'vulcan': 3,\n",
       "             'ear': 116,\n",
       "             'utility': 15,\n",
       "             'tricorder': 1,\n",
       "             'light': 275,\n",
       "             'saber': 9,\n",
       "             'dude': 206,\n",
       "             'otto': 80,\n",
       "             'oooh': 141,\n",
       "             'comic_book': 77,\n",
       "             'school': 970,\n",
       "             'bus': 198,\n",
       "             'vampire': 51,\n",
       "             'post': 68,\n",
       "             'apocalyptic': 2,\n",
       "             'warzone': 1,\n",
       "             \"c'mon\": 430,\n",
       "             'buddy': 148,\n",
       "             'hodge': 2,\n",
       "             'play': 876,\n",
       "             'tv': 419,\n",
       "             'kill': 796,\n",
       "             'vietnam': 18,\n",
       "             'aaahh': 1,\n",
       "             'laramie': 15,\n",
       "             'cigarette': 71,\n",
       "             'steady': 23,\n",
       "             'combat': 13,\n",
       "             'evil': 142,\n",
       "             'whilliker': 2,\n",
       "             'wisht': 1,\n",
       "             'sixteen': 33,\n",
       "             'earth': 182,\n",
       "             \"y'know\": 87,\n",
       "             'actor': 51,\n",
       "             'dirk': 4,\n",
       "             'richter': 3,\n",
       "             'portrayal': 1,\n",
       "             'sordid': 6,\n",
       "             'detail': 26,\n",
       "             'question': 312,\n",
       "             'tasteful': 5,\n",
       "             'inject': 11,\n",
       "             'shrink': 10,\n",
       "             'serum': 4,\n",
       "             'issue': 74,\n",
       "             'finish': 189,\n",
       "             'tum': 6,\n",
       "             'tugger': 1,\n",
       "             'second': 414,\n",
       "             'national': 69,\n",
       "             'tour': 93,\n",
       "             'company': 171,\n",
       "             'cat': 242,\n",
       "             'ooh_ooh': 34,\n",
       "             'masked': 5,\n",
       "             'haunt': 30,\n",
       "             'bordello': 3,\n",
       "             'bullet': 58,\n",
       "             'riddled': 1,\n",
       "             'body': 209,\n",
       "             'vulture': 7,\n",
       "             'seventy': 89,\n",
       "             'imaginary': 21,\n",
       "             'tale': 58,\n",
       "             'marrie': 3,\n",
       "             'larva': 2,\n",
       "             'grubby': 2,\n",
       "             '-PRON-': 575,\n",
       "             'bet': 252,\n",
       "             'million': 156,\n",
       "             'buck': 220,\n",
       "             'lad': 38,\n",
       "             'remind': 124,\n",
       "             'moment': 209,\n",
       "             'sell': 394,\n",
       "             'dozen': 27,\n",
       "             'lois': 7,\n",
       "             'lane': 33,\n",
       "             'superman': 28,\n",
       "             'see': 714,\n",
       "             'lariat': 1,\n",
       "             'dinner': 311,\n",
       "             'treat': 139,\n",
       "             'sport': 133,\n",
       "             'fine': 579,\n",
       "             'restaurant': 86,\n",
       "             'draw': 100,\n",
       "             'micha': 1,\n",
       "             'langelo': 1,\n",
       "             'usually': 95,\n",
       "             'bug': 108,\n",
       "             'mad': 240,\n",
       "             'pay_attention': 48,\n",
       "             'win': 644,\n",
       "             'apple': 136,\n",
       "             'woo_hoo': 204,\n",
       "             'gloat': 2,\n",
       "             'age': 170,\n",
       "             'size': 106,\n",
       "             'electric': 64,\n",
       "             'lightbulb': 4,\n",
       "             'oven': 32,\n",
       "             'patty_selma': 31,\n",
       "             'allowance': 14,\n",
       "             'slave': 37,\n",
       "             'free': 536,\n",
       "             'smoking': 22,\n",
       "             'month': 252,\n",
       "             'venus': 14,\n",
       "             'shield': 13,\n",
       "             'wash': 116,\n",
       "             'drip': 12,\n",
       "             'finally': 360,\n",
       "             'extra': 170,\n",
       "             'answer': 298,\n",
       "             'aw': 485,\n",
       "             'piece': 177,\n",
       "             'childhood': 31,\n",
       "             'ahh': 72,\n",
       "             'hm': 255,\n",
       "             'practically': 21,\n",
       "             'ooh': 536,\n",
       "             'deposit': 21,\n",
       "             'defray': 1,\n",
       "             'cost': 145,\n",
       "             'jumbo': 19,\n",
       "             'squishie': 1,\n",
       "             'dime': 29,\n",
       "             'learn': 495,\n",
       "             'trade': 69,\n",
       "             'americanize': 1,\n",
       "             'coin': 37,\n",
       "             'cent': 125,\n",
       "             'humiliating': 11,\n",
       "             'terrible': 181,\n",
       "             'slow': 106,\n",
       "             'laugh': 262,\n",
       "             'buying': 3,\n",
       "             'sympathy': 5,\n",
       "             'ha': 168,\n",
       "             'pathetic': 44,\n",
       "             'lemonade': 28,\n",
       "             'suck': 209,\n",
       "             'product': 67,\n",
       "             'form': 128,\n",
       "             'crowd': 84,\n",
       "             'cheap': 86,\n",
       "             'beer': 513,\n",
       "             'sympathetic': 4,\n",
       "             'credit': 78,\n",
       "             'liquor': 45,\n",
       "             'license': 76,\n",
       "             'ugh': 58,\n",
       "             'dog': 665,\n",
       "             'ticket': 219,\n",
       "             'thirsty': 13,\n",
       "             'ell': 3,\n",
       "             'offense': 28,\n",
       "             'officer': 68,\n",
       "             'poor': 229,\n",
       "             'earn': 82,\n",
       "             'nazi': 12,\n",
       "             'smasher': 2,\n",
       "             'chore': 27,\n",
       "             'mix': 70,\n",
       "             'whitewash': 4,\n",
       "             'eh': 515,\n",
       "             'burt': 12,\n",
       "             'apricot': 3,\n",
       "             'almond': 12,\n",
       "             'paste': 14,\n",
       "             'sauerkraut': 4,\n",
       "             'candy': 198,\n",
       "             'brother': 332,\n",
       "             'asa': 7,\n",
       "             'grenade': 6,\n",
       "             'kaiser': 4,\n",
       "             'bill': 217,\n",
       "             'delivery': 42,\n",
       "             'uncle': 92,\n",
       "             'sam': 20,\n",
       "             'harrison': 6,\n",
       "             'brooklyn': 9,\n",
       "             'bob': 167,\n",
       "             'reggie': 3,\n",
       "             'stuck': 43,\n",
       "             'ribbon': 24,\n",
       "             'madam': 110,\n",
       "             'start': 805,\n",
       "             'yard': 86,\n",
       "             'barley': 3,\n",
       "             'pop': 137,\n",
       "             'weed': 12,\n",
       "             'one': 133,\n",
       "             'careful': 76,\n",
       "             'watch': 716,\n",
       "             ...})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leland',\n",
       " 'promontory',\n",
       " 'parkville',\n",
       " 'mopey',\n",
       " 'doover',\n",
       " 'mixology',\n",
       " 'planter',\n",
       " 'cassis',\n",
       " 'ssslurr',\n",
       " 'shpeech']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'not', 'oh', 'will', 'like', \"'s\", 'know', 'think', 'hey', 'good']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "print(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The parameters:\n",
    "## min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "## window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
    "## size = int - Dimensionality of the feature vectors. - (50, 300)\n",
    "## sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
    "## alpha = float - The initial learning rate - (0.01, 0.05)\n",
    "## min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "## negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "## workers = int - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20, \n",
    "                     window=2, \n",
    "                     size=300, \n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:28:31: collecting all words and their counts\n",
      "INFO - 21:28:31: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 21:28:31: PROGRESS: at sentence #10000, processed 65193 words, keeping 9096 word types\n",
      "INFO - 21:28:32: PROGRESS: at sentence #20000, processed 136024 words, keeping 13916 word types\n",
      "INFO - 21:28:32: PROGRESS: at sentence #30000, processed 201577 words, keeping 16865 word types\n",
      "INFO - 21:28:32: PROGRESS: at sentence #40000, processed 262082 words, keeping 19506 word types\n",
      "INFO - 21:28:32: PROGRESS: at sentence #50000, processed 324069 words, keeping 21758 word types\n",
      "INFO - 21:28:32: PROGRESS: at sentence #60000, processed 388895 words, keeping 23910 word types\n",
      "INFO - 21:28:33: PROGRESS: at sentence #70000, processed 454042 words, keeping 25876 word types\n",
      "INFO - 21:28:33: PROGRESS: at sentence #80000, processed 518929 words, keeping 27769 word types\n",
      "INFO - 21:28:33: PROGRESS: at sentence #90000, processed 584755 words, keeping 29345 word types\n",
      "INFO - 21:28:33: collected 29673 word types from a corpus of 601119 raw words and 92412 sentences\n",
      "INFO - 21:28:33: Loading a fresh vocabulary\n",
      "INFO - 21:28:33: effective_min_count=20 retains 3375 unique words (11% of original 29673, drops 26298)\n",
      "INFO - 21:28:33: effective_min_count=20 leaves 515089 word corpus (85% of original 601119, drops 86030)\n",
      "INFO - 21:28:33: deleting the raw counts dictionary of 29673 items\n",
      "INFO - 21:28:33: sample=6e-05 downsamples 1080 most-common words\n",
      "INFO - 21:28:33: downsampling leaves estimated 219321 word corpus (42.6% of prior 515089)\n",
      "INFO - 21:28:33: estimated required memory for 3375 words and 300 dimensions: 9787500 bytes\n",
      "INFO - 21:28:33: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.05 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model:\n",
    "\n",
    "## Parameters of the training:\n",
    "\n",
    "## total_examples = int - Count of sentences;\n",
    "## epochs = int - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:29:40: training model with 7 workers on 3375 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2\n",
      "INFO - 21:29:41: EPOCH 1 - PROGRESS: at 37.39% examples, 82785 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:42: EPOCH 1 - PROGRESS: at 66.81% examples, 71346 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:42: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:29:42: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:29:42: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:29:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:29:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:42: EPOCH - 1 : training on 601119 raw words (219268 effective words) took 2.8s, 76947 effective words/s\n",
      "INFO - 21:29:43: EPOCH 2 - PROGRESS: at 40.98% examples, 86644 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:45: EPOCH 2 - PROGRESS: at 80.01% examples, 84238 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:29:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:29:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:29:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:29:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:45: EPOCH - 2 : training on 601119 raw words (219459 effective words) took 2.6s, 85211 effective words/s\n",
      "INFO - 21:29:46: EPOCH 3 - PROGRESS: at 40.98% examples, 88042 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:47: EPOCH 3 - PROGRESS: at 68.49% examples, 72870 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:48: EPOCH 3 - PROGRESS: at 98.21% examples, 70065 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:48: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:29:48: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:29:48: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:29:48: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:29:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:48: EPOCH - 3 : training on 601119 raw words (218941 effective words) took 3.1s, 70208 effective words/s\n",
      "INFO - 21:29:49: EPOCH 4 - PROGRESS: at 40.98% examples, 88991 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:50: EPOCH 4 - PROGRESS: at 81.64% examples, 88358 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:51: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:29:51: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:29:51: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:29:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:29:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:51: EPOCH - 4 : training on 601119 raw words (219933 effective words) took 2.4s, 89999 effective words/s\n",
      "INFO - 21:29:52: EPOCH 5 - PROGRESS: at 40.98% examples, 90542 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:53: EPOCH 5 - PROGRESS: at 84.97% examples, 92016 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:53: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:29:53: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:29:53: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:29:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:29:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:53: EPOCH - 5 : training on 601119 raw words (219201 effective words) took 2.9s, 76893 effective words/s\n",
      "INFO - 21:29:55: EPOCH 6 - PROGRESS: at 39.12% examples, 85714 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:56: EPOCH 6 - PROGRESS: at 76.68% examples, 83130 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:57: EPOCH 6 - PROGRESS: at 95.04% examples, 68708 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:29:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:29:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:29:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:29:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:57: EPOCH - 6 : training on 601119 raw words (219677 effective words) took 3.2s, 69733 effective words/s\n",
      "INFO - 21:29:58: EPOCH 7 - PROGRESS: at 25.38% examples, 55884 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:59: EPOCH 7 - PROGRESS: at 61.73% examples, 63336 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:00: EPOCH 7 - PROGRESS: at 93.44% examples, 64627 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:00: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:00: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:00: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:00: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:00: EPOCH - 7 : training on 601119 raw words (219546 effective words) took 3.3s, 65901 effective words/s\n",
      "INFO - 21:30:01: EPOCH 8 - PROGRESS: at 25.38% examples, 48909 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:02: EPOCH 8 - PROGRESS: at 53.41% examples, 52212 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:03: EPOCH 8 - PROGRESS: at 76.68% examples, 51438 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:04: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:04: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:04: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:04: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:04: EPOCH 8 - PROGRESS: at 98.41% examples, 50621 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 21:30:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:04: EPOCH - 8 : training on 601119 raw words (219363 effective words) took 4.3s, 50873 effective words/s\n",
      "INFO - 21:30:05: EPOCH 9 - PROGRESS: at 28.77% examples, 64673 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:06: EPOCH 9 - PROGRESS: at 61.73% examples, 66580 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:07: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:07: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:07: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:07: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:07: EPOCH - 9 : training on 601119 raw words (219047 effective words) took 2.9s, 74629 effective words/s\n",
      "INFO - 21:30:08: EPOCH 10 - PROGRESS: at 39.12% examples, 86864 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:30:09: EPOCH 10 - PROGRESS: at 68.49% examples, 72815 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:10: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:10: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:10: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:10: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:10: EPOCH - 10 : training on 601119 raw words (219194 effective words) took 2.8s, 79422 effective words/s\n",
      "INFO - 21:30:11: EPOCH 11 - PROGRESS: at 42.87% examples, 91338 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:12: EPOCH 11 - PROGRESS: at 86.72% examples, 92369 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:12: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:12: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:12: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:12: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:12: EPOCH - 11 : training on 601119 raw words (219482 effective words) took 2.4s, 92280 effective words/s\n",
      "INFO - 21:30:13: EPOCH 12 - PROGRESS: at 40.98% examples, 87656 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:15: EPOCH 12 - PROGRESS: at 71.73% examples, 73294 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:16: EPOCH - 12 : training on 601119 raw words (219283 effective words) took 3.1s, 70269 effective words/s\n",
      "INFO - 21:30:17: EPOCH 13 - PROGRESS: at 40.98% examples, 89233 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:18: EPOCH 13 - PROGRESS: at 84.97% examples, 91597 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:18: EPOCH - 13 : training on 601119 raw words (219163 effective words) took 2.4s, 92116 effective words/s\n",
      "INFO - 21:30:19: EPOCH 14 - PROGRESS: at 22.26% examples, 48148 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:20: EPOCH 14 - PROGRESS: at 65.09% examples, 68279 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:21: EPOCH 14 - PROGRESS: at 95.04% examples, 67655 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:21: EPOCH - 14 : training on 601119 raw words (219808 effective words) took 3.2s, 68805 effective words/s\n",
      "INFO - 21:30:22: EPOCH 15 - PROGRESS: at 39.12% examples, 83778 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:23: EPOCH 15 - PROGRESS: at 73.39% examples, 78018 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:24: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:24: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:24: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:24: EPOCH - 15 : training on 601119 raw words (219285 effective words) took 2.8s, 78944 effective words/s\n",
      "INFO - 21:30:25: EPOCH 16 - PROGRESS: at 39.12% examples, 86570 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:26: EPOCH 16 - PROGRESS: at 81.64% examples, 87221 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:26: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:26: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:26: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:26: EPOCH - 16 : training on 601119 raw words (219332 effective words) took 2.5s, 87703 effective words/s\n",
      "INFO - 21:30:28: EPOCH 17 - PROGRESS: at 28.77% examples, 56110 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:29: EPOCH 17 - PROGRESS: at 71.73% examples, 71081 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:29: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:29: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:29: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:29: EPOCH - 17 : training on 601119 raw words (219136 effective words) took 3.0s, 73417 effective words/s\n",
      "INFO - 21:30:30: EPOCH 18 - PROGRESS: at 37.39% examples, 80465 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:31: EPOCH 18 - PROGRESS: at 78.38% examples, 83652 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:32: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:32: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:32: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:32: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:32: EPOCH - 18 : training on 601119 raw words (219590 effective words) took 2.6s, 84198 effective words/s\n",
      "INFO - 21:30:33: EPOCH 19 - PROGRESS: at 39.12% examples, 85363 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:34: EPOCH 19 - PROGRESS: at 65.09% examples, 70106 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:35: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:35: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:35: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:35: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:35: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:30:35: EPOCH - 19 : training on 601119 raw words (219133 effective words) took 2.9s, 75832 effective words/s\n",
      "INFO - 21:30:36: EPOCH 20 - PROGRESS: at 39.12% examples, 85505 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:37: EPOCH 20 - PROGRESS: at 80.01% examples, 85963 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:37: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:37: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:37: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:37: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:38: EPOCH - 20 : training on 601119 raw words (219339 effective words) took 2.5s, 86091 effective words/s\n",
      "INFO - 21:30:39: EPOCH 21 - PROGRESS: at 39.12% examples, 84350 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:40: EPOCH 21 - PROGRESS: at 68.49% examples, 72390 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:40: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:40: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:40: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:40: EPOCH - 21 : training on 601119 raw words (219370 effective words) took 2.9s, 76725 effective words/s\n",
      "INFO - 21:30:41: EPOCH 22 - PROGRESS: at 39.12% examples, 85675 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:42: EPOCH 22 - PROGRESS: at 81.64% examples, 87016 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:43: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:43: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:43: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:43: EPOCH - 22 : training on 601119 raw words (219411 effective words) took 2.5s, 86893 effective words/s\n",
      "INFO - 21:30:44: EPOCH 23 - PROGRESS: at 39.12% examples, 86091 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:45: EPOCH 23 - PROGRESS: at 76.68% examples, 73895 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:46: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:46: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:46: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:46: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:46: EPOCH - 23 : training on 601119 raw words (219541 effective words) took 2.8s, 77084 effective words/s\n",
      "INFO - 21:30:47: EPOCH 24 - PROGRESS: at 39.12% examples, 83773 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:48: EPOCH 24 - PROGRESS: at 80.01% examples, 85640 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:48: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:48: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:48: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:48: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:48: EPOCH - 24 : training on 601119 raw words (219026 effective words) took 2.5s, 86530 effective words/s\n",
      "INFO - 21:30:49: EPOCH 25 - PROGRESS: at 39.12% examples, 84107 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:50: EPOCH 25 - PROGRESS: at 81.64% examples, 86435 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:51: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:51: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:51: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:51: EPOCH - 25 : training on 601119 raw words (219369 effective words) took 2.9s, 76965 effective words/s\n",
      "INFO - 21:30:52: EPOCH 26 - PROGRESS: at 39.12% examples, 85518 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:53: EPOCH 26 - PROGRESS: at 81.64% examples, 88080 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:54: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:54: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:54: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:54: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:54: EPOCH - 26 : training on 601119 raw words (219095 effective words) took 2.5s, 86083 effective words/s\n",
      "INFO - 21:30:55: EPOCH 27 - PROGRESS: at 40.98% examples, 88583 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:56: EPOCH 27 - PROGRESS: at 80.01% examples, 85818 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:56: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:56: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:56: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:56: EPOCH - 27 : training on 601119 raw words (219584 effective words) took 2.6s, 85565 effective words/s\n",
      "INFO - 21:30:57: EPOCH 28 - PROGRESS: at 25.38% examples, 52409 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:58: EPOCH 28 - PROGRESS: at 65.09% examples, 67292 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:59: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:30:59: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:30:59: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:30:59: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:30:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:59: EPOCH - 28 : training on 601119 raw words (219753 effective words) took 3.1s, 71993 effective words/s\n",
      "INFO - 21:31:00: EPOCH 29 - PROGRESS: at 28.77% examples, 64456 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:31:01: EPOCH 29 - PROGRESS: at 68.49% examples, 73311 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:31:02: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:31:02: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:31:02: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:31:02: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:31:02: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:31:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:31:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:31:02: EPOCH - 29 : training on 601119 raw words (219058 effective words) took 2.8s, 77158 effective words/s\n",
      "INFO - 21:31:03: EPOCH 30 - PROGRESS: at 35.62% examples, 78451 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:31:04: EPOCH 30 - PROGRESS: at 63.43% examples, 68119 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:31:05: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:31:05: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:31:05: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:31:05: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:31:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:31:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:31:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:31:05: EPOCH - 30 : training on 601119 raw words (219783 effective words) took 2.9s, 75711 effective words/s\n",
      "INFO - 21:31:05: training on a 18033570 raw words (6581170 effective words) took 85.6s, 76922 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 1.43 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we do not plan to train the model any further, we are calling init_sims(), which will make the model much more memory-efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:32:42: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec on Reddit comments (ML Course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/swetha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# IPython-related imports\n",
    "from IPython.display import HTML\n",
    "\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "# Removing special charecters and tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../kaggle_compettition/ift3395-ift6390-reddit-comments/data_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/swetha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|| 70000/70000 [00:09<00:00, 7342.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import string\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmtizer = WordNetLemmatizer()\n",
    "\n",
    "trset = []\n",
    "for i in trange(len(train[0])):\n",
    "    trset.append(train[0][i])\n",
    "    trset[i] = trset[i].lower()\n",
    "    trset[i] = tokenizer.tokenize(trset[i])\n",
    "    trset[i] = [i for i in trset[i] if not i in stop_words]\n",
    "    trset[i] = [lemmtizer.lemmatize(w) for w in trset[i]]\n",
    "    \n",
    "    trset[i] = \" \".join(trset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# As Phrases() takes a list of list of words as input:\n",
    "\n",
    "sent = [row.split() for row in trset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['honestly',\n",
       " 'buffalo',\n",
       " 'correct',\n",
       " 'answer',\n",
       " 'remember',\n",
       " 'people',\n",
       " 'somewhat',\n",
       " 'joking',\n",
       " 'buffalo',\n",
       " 'mantra',\n",
       " 'starting',\n",
       " 'goalie',\n",
       " 'win',\n",
       " 'game',\n",
       " 'get',\n",
       " 'traded',\n",
       " 'think',\n",
       " 'edmonton',\n",
       " 'front',\n",
       " 'office',\n",
       " 'travesty',\n",
       " 'better',\n",
       " 'part',\n",
       " '10',\n",
       " 'year',\n",
       " 'buffalo',\n",
       " 'systematic',\n",
       " 'destruction',\n",
       " 'term',\n",
       " 'competitive',\n",
       " 'much',\n",
       " 'responsible',\n",
       " 'change',\n",
       " 'draft',\n",
       " 'lottery']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:19:42: collecting all words and their counts\n",
      "INFO - 11:19:42: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 11:19:42: PROGRESS: at sentence #10000, processed 223948 words and 195676 word types\n",
      "INFO - 11:19:42: PROGRESS: at sentence #20000, processed 454311 words and 359352 word types\n",
      "INFO - 11:19:43: PROGRESS: at sentence #30000, processed 680045 words and 508758 word types\n",
      "INFO - 11:19:43: PROGRESS: at sentence #40000, processed 912738 words and 653432 word types\n",
      "INFO - 11:19:44: PROGRESS: at sentence #50000, processed 1143566 words and 789416 word types\n",
      "INFO - 11:19:44: PROGRESS: at sentence #60000, processed 1369541 words and 919140 word types\n",
      "INFO - 11:19:45: collected 1042724 word types from a corpus of 1593639 words (unigram + bigrams) and 70000 sentences\n",
      "INFO - 11:19:45: using 1042724 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:19:45: source_vocab length 1042724\n",
      "INFO - 11:19:55: Phraser built with 873 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68560"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'honestly': 695,\n",
       "             'buffalo': 70,\n",
       "             'correct': 369,\n",
       "             'answer': 495,\n",
       "             'remember': 1145,\n",
       "             'people': 9163,\n",
       "             'somewhat': 157,\n",
       "             'joking': 62,\n",
       "             'mantra': 7,\n",
       "             'starting': 432,\n",
       "             'goalie': 126,\n",
       "             'win': 1442,\n",
       "             'game': 5494,\n",
       "             'get': 9494,\n",
       "             'traded': 158,\n",
       "             'think': 8040,\n",
       "             'edmonton': 48,\n",
       "             'front_office': 40,\n",
       "             'travesty': 10,\n",
       "             'better': 3379,\n",
       "             'part': 1874,\n",
       "             '10_year': 168,\n",
       "             'systematic': 7,\n",
       "             'destruction': 54,\n",
       "             'term': 564,\n",
       "             'competitive': 273,\n",
       "             'much': 4072,\n",
       "             'responsible': 123,\n",
       "             'change': 1360,\n",
       "             'draft': 214,\n",
       "             'lottery': 45,\n",
       "             'ah': 209,\n",
       "             'yes': 1532,\n",
       "             'way': 4439,\n",
       "             'could': 4048,\n",
       "             'drafted': 105,\n",
       "             'thought': 1890,\n",
       "             'gonna': 858,\n",
       "             'great': 2356,\n",
       "             'nope': 177,\n",
       "             'kawhi': 44,\n",
       "             'thompson': 32,\n",
       "             'jimmy': 54,\n",
       "             'butler': 41,\n",
       "             'http_youtu': 221,\n",
       "             '6xxbbr8isz0': 1,\n",
       "             '40m49s': 1,\n",
       "             'find': 1787,\n",
       "             'already': 1539,\n",
       "             'nothing': 1676,\n",
       "             'ordinary': 24,\n",
       "             'though': 2608,\n",
       "             'eye': 495,\n",
       "             'constant': 98,\n",
       "             'contact': 187,\n",
       "             'bad': 2509,\n",
       "             'signing': 207,\n",
       "             'paid': 433,\n",
       "             '18m': 6,\n",
       "             'euro': 93,\n",
       "             'right': 3935,\n",
       "             'price': 523,\n",
       "             'would': 9696,\n",
       "             'acceptable': 102,\n",
       "             'easy': 653,\n",
       "             'use': 2165,\n",
       "             'piss': 126,\n",
       "             'dry': 83,\n",
       "             'technique': 58,\n",
       "             'let': 1649,\n",
       "             'drop': 421,\n",
       "             'rinse': 16,\n",
       "             'repeat': 127,\n",
       "             'lucky': 206,\n",
       "             'end': 1842,\n",
       "             'using': 958,\n",
       "             'hot': 337,\n",
       "             'sunny': 14,\n",
       "             'day': 2392,\n",
       "             'process': 286,\n",
       "             'go': 4233,\n",
       "             'lot': 3301,\n",
       "             'faster': 158,\n",
       "             'joke': 627,\n",
       "             'seen': 1020,\n",
       "             'twice': 235,\n",
       "             'role': 464,\n",
       "             'mi3': 2,\n",
       "             'one': 10124,\n",
       "             'best': 2741,\n",
       "             'villians': 6,\n",
       "             'movie': 1980,\n",
       "             'genuinely': 112,\n",
       "             'felt_like': 162,\n",
       "             'give_shit': 110,\n",
       "             'loved': 385,\n",
       "             'inflict': 8,\n",
       "             'pain': 216,\n",
       "             'wanted': 826,\n",
       "             'nailed': 21,\n",
       "             'lack': 365,\n",
       "             'remorse': 10,\n",
       "             'love': 2208,\n",
       "             'also': 5352,\n",
       "             'help': 1403,\n",
       "             'story': 1235,\n",
       "             'centered': 17,\n",
       "             'becoming': 178,\n",
       "             'personal': 386,\n",
       "             'hunt': 45,\n",
       "             'woman': 836,\n",
       "             'life': 1600,\n",
       "             'akagi': 1,\n",
       "             'still': 4397,\n",
       "             'alpha': 116,\n",
       "             'fuck': 1726,\n",
       "             'sugawara': 1,\n",
       "             'suffering': 83,\n",
       "             'definitely': 1116,\n",
       "             'two': 2262,\n",
       "             'favorite': 520,\n",
       "             'couple': 488,\n",
       "             'far': 1496,\n",
       "             'detonator': 1,\n",
       "             'proven': 116,\n",
       "             'joker': 25,\n",
       "             'blew': 59,\n",
       "             'boat': 125,\n",
       "             'boatload': 4,\n",
       "             'survivor': 27,\n",
       "             'attest': 7,\n",
       "             'pulled': 145,\n",
       "             'trigger': 88,\n",
       "             'disruptor': 5,\n",
       "             'tank': 640,\n",
       "             'pull': 318,\n",
       "             'dp': 471,\n",
       "             'frey': 50,\n",
       "             'pick': 887,\n",
       "             'point': 3171,\n",
       "             'reliably': 11,\n",
       "             'hero': 672,\n",
       "             'expect': 463,\n",
       "             'heartbreaking': 19,\n",
       "             'flying': 131,\n",
       "             'eagle': 101,\n",
       "             'mordor': 2,\n",
       "             'thing': 4969,\n",
       "             'incredibly': 208,\n",
       "             'divisive': 11,\n",
       "             'heard': 593,\n",
       "             'fight': 836,\n",
       "             'back_forth': 37,\n",
       "             'whether': 440,\n",
       "             'plot': 258,\n",
       "             'hole': 212,\n",
       "             'looking': 943,\n",
       "             'insight': 47,\n",
       "             'oh': 1069,\n",
       "             'man': 1795,\n",
       "             'wait': 750,\n",
       "             'vote': 558,\n",
       "             'open': 549,\n",
       "             'link': 663,\n",
       "             'start': 1400,\n",
       "             'sweating': 8,\n",
       "             'omg': 69,\n",
       "             'thinking': 668,\n",
       "             'azumi': 2,\n",
       "             'u': 3952,\n",
       "             'af': 62,\n",
       "             'boi': 20,\n",
       "             'shot': 839,\n",
       "             'kill': 1094,\n",
       "             'character': 1429,\n",
       "             'new': 2469,\n",
       "             'sub': 664,\n",
       "             'curious': 173,\n",
       "             '1': 2741,\n",
       "             '10': 1366,\n",
       "             'describing': 28,\n",
       "             'high': 1259,\n",
       "             'pay': 826,\n",
       "             '220': 5,\n",
       "             'oz': 15,\n",
       "             'brooklyn': 31,\n",
       "             'mid': 363,\n",
       "             'range': 297,\n",
       "             'tree': 225,\n",
       "             'grade': 130,\n",
       "             'run': 1086,\n",
       "             '300': 136,\n",
       "             '450': 10,\n",
       "             'glad': 325,\n",
       "             'considering': 376,\n",
       "             'rewatch': 56,\n",
       "             'series': 635,\n",
       "             'perfect': 382,\n",
       "             'certainly': 391,\n",
       "             'enjoyable': 62,\n",
       "             'personally': 370,\n",
       "             'like': 11054,\n",
       "             'subaru': 8,\n",
       "             'interesting': 653,\n",
       "             'added': 223,\n",
       "             'cast': 271,\n",
       "             'later': 596,\n",
       "             'well': 4189,\n",
       "             'window': 242,\n",
       "             'player': 3567,\n",
       "             'club': 502,\n",
       "             'free': 835,\n",
       "             'back': 2740,\n",
       "             'page': 328,\n",
       "             'fodder': 11,\n",
       "             'tabloid': 20,\n",
       "             'big': 1613,\n",
       "             'shrug': 17,\n",
       "             'willing': 282,\n",
       "             'negotiate': 21,\n",
       "             'contract': 605,\n",
       "             'transfer': 203,\n",
       "             'ban': 289,\n",
       "             'valid': 125,\n",
       "             'reason': 1758,\n",
       "             'lol': 1414,\n",
       "             'mate': 161,\n",
       "             'trying': 1516,\n",
       "             'defend': 216,\n",
       "             'incompetent': 46,\n",
       "             'barca': 130,\n",
       "             'board': 211,\n",
       "             'bunch': 365,\n",
       "             'corrupt': 79,\n",
       "             'exactly': 891,\n",
       "             'good': 5541,\n",
       "             'businessmen': 3,\n",
       "             'past': 744,\n",
       "             'know': 5724,\n",
       "             'incompetence': 21,\n",
       "             'showed': 200,\n",
       "             'said': 2609,\n",
       "             'afraid': 127,\n",
       "             'addicted': 27,\n",
       "             'fail': 156,\n",
       "             'school': 593,\n",
       "             'smthing': 1,\n",
       "             'shh': 3,\n",
       "             'baby': 275,\n",
       "             'okay': 451,\n",
       "             'got': 3541,\n",
       "             'ring': 232,\n",
       "             'ever': 1587,\n",
       "             'first': 3070,\n",
       "             'half': 891,\n",
       "             'another': 1613,\n",
       "             'sam': 130,\n",
       "             'adam': 96,\n",
       "             'calm': 85,\n",
       "             'trampoline': 2,\n",
       "             'centre': 56,\n",
       "             'sister': 227,\n",
       "             'birthday': 64,\n",
       "             'sat': 71,\n",
       "             'coffee': 117,\n",
       "             'session': 101,\n",
       "             'comment': 1945,\n",
       "             'really': 6101,\n",
       "             'burn': 188,\n",
       "             'someone': 2242,\n",
       "             'table': 171,\n",
       "             'next': 1107,\n",
       "             'mine': 302,\n",
       "             'slightly': 214,\n",
       "             'sloped': 4,\n",
       "             'put': 1728,\n",
       "             'chair': 60,\n",
       "             'waiter': 27,\n",
       "             'since': 2133,\n",
       "             'saw': 855,\n",
       "             'tip': 225,\n",
       "             'leap': 27,\n",
       "             'clean': 186,\n",
       "             'super': 605,\n",
       "             'lap': 19,\n",
       "             'screaming': 84,\n",
       "             'scalding': 1,\n",
       "             'mum': 17,\n",
       "             'freak': 63,\n",
       "             'knowing': 251,\n",
       "             'happened': 827,\n",
       "             'stand': 449,\n",
       "             'grab': 94,\n",
       "             'shout': 30,\n",
       "             'rip': 145,\n",
       "             'soaked': 10,\n",
       "             'romper': 3,\n",
       "             'hand': 877,\n",
       "             'jog': 4,\n",
       "             'fridge': 18,\n",
       "             '3': 2352,\n",
       "             'bottle': 108,\n",
       "             'cold': 174,\n",
       "             'water': 429,\n",
       "             'pour': 30,\n",
       "             'holding': 169,\n",
       "             'tell': 1177,\n",
       "             'stunned': 15,\n",
       "             'staff': 158,\n",
       "             'call': 996,\n",
       "             '000': 274,\n",
       "             'report': 397,\n",
       "             'bring': 451,\n",
       "             'wet': 46,\n",
       "             'towel': 24,\n",
       "             'see': 4901,\n",
       "             'skin': 390,\n",
       "             'red': 439,\n",
       "             'loosely': 9,\n",
       "             'wrap': 52,\n",
       "             'cloth': 21,\n",
       "             'arm': 259,\n",
       "             'making': 1418,\n",
       "             'happy': 584,\n",
       "             'noise': 86,\n",
       "             'collect': 43,\n",
       "             'walk': 341,\n",
       "             'friend': 1376,\n",
       "             'walked': 100,\n",
       "             'ambulance': 7,\n",
       "             'pulling': 100,\n",
       "             'yeah': 2225,\n",
       "             'saving': 130,\n",
       "             'edit': 1103,\n",
       "             'teacher': 154,\n",
       "             'must': 809,\n",
       "             'comprehensive': 10,\n",
       "             'aid': 90,\n",
       "             'course': 827,\n",
       "             'every': 2114,\n",
       "             'year': 3816,\n",
       "             'cpr': 10,\n",
       "             'epipen': 1,\n",
       "             '6_month': 69,\n",
       "             'guess': 1091,\n",
       "             'surely': 139,\n",
       "             'defensive': 216,\n",
       "             'midfielder': 20,\n",
       "             'buy': 836,\n",
       "             'papua': 1,\n",
       "             'guinea': 4,\n",
       "             'miserly': 1,\n",
       "             'sum': 69,\n",
       "             '80': 343,\n",
       "             'mill': 50,\n",
       "             'scandinavian': 14,\n",
       "             'version': 453,\n",
       "             'jermain': 1,\n",
       "             'jena': 1,\n",
       "             'swansea': 9,\n",
       "             '50': 637,\n",
       "             'oh_wait': 60,\n",
       "             'entire': 717,\n",
       "             'market': 451,\n",
       "             'full': 980,\n",
       "             'inflated': 27,\n",
       "             'bog': 4,\n",
       "             'standard': 337,\n",
       "             'average': 607,\n",
       "             'spur': 106,\n",
       "             'splash': 24,\n",
       "             'simply': 596,\n",
       "             'show': 2591,\n",
       "             'ambition': 12,\n",
       "             'live': 1168,\n",
       "             'shanghainese': 1,\n",
       "             'girlfriend': 109,\n",
       "             'agree': 1114,\n",
       "             'op': 549,\n",
       "             'gold': 434,\n",
       "             'play': 3586,\n",
       "             'support': 917,\n",
       "             'shield': 182,\n",
       "             'item': 325,\n",
       "             'wont': 150,\n",
       "             'laneing': 1,\n",
       "             'phase': 105,\n",
       "             'laners': 21,\n",
       "             'never': 2967,\n",
       "             'last': 1629,\n",
       "             'hit': 1384,\n",
       "             'burst': 98,\n",
       "             'wave': 150,\n",
       "             'fast': 349,\n",
       "             'problem': 1594,\n",
       "             'spoiler_warning': 144,\n",
       "             'main_spoiler': 113,\n",
       "             'mean': 3042,\n",
       "             'event_published': 37,\n",
       "             'book_aired': 37,\n",
       "             'episode': 830,\n",
       "             'need_spoiler': 141,\n",
       "             'tag_future': 37,\n",
       "             'book': 909,\n",
       "             'season': 2135,\n",
       "             'spoiler': 392,\n",
       "             'medium_covered': 38,\n",
       "             'info_please': 97,\n",
       "             'check_spoiler': 97,\n",
       "             'guide_r': 97,\n",
       "             'gameofthrones_w': 97,\n",
       "             'spoiler_guide_bot': 97,\n",
       "             'action_performed': 177,\n",
       "             'automatically_please': 172,\n",
       "             'contact_moderator': 181,\n",
       "             'subreddit_message': 172,\n",
       "             'compose_r': 218,\n",
       "             'gameofthrones_question': 116,\n",
       "             'concern': 333,\n",
       "             'want': 4269,\n",
       "             'isi': 92,\n",
       "             'charge': 350,\n",
       "             'keep': 1538,\n",
       "             'region': 229,\n",
       "             'unstable': 28,\n",
       "             'damn': 463,\n",
       "             'playing': 1539,\n",
       "             'level': 1171,\n",
       "             'living': 427,\n",
       "             'cote': 2,\n",
       "             'azur': 1,\n",
       "             'choice': 408,\n",
       "             'wes': 9,\n",
       "             'fuckin': 95,\n",
       "             'n0thing': 8,\n",
       "             'tap': 46,\n",
       "             'take': 2948,\n",
       "             'shirt': 142,\n",
       "             'straight': 393,\n",
       "             'g': 158,\n",
       "             'maybe': 2009,\n",
       "             'listening': 208,\n",
       "             'telephone': 8,\n",
       "             'conversation': 285,\n",
       "             'pirate': 77,\n",
       "             'music': 1402,\n",
       "             'industry': 200,\n",
       "             'force': 507,\n",
       "             'reinforcing': 3,\n",
       "             'corruption': 94,\n",
       "             'elsewhere': 95,\n",
       "             'artist': 400,\n",
       "             'fairly': 223,\n",
       "             'probably': 2468,\n",
       "             'stop': 1014,\n",
       "             'type': 595,\n",
       "             'discussion': 429,\n",
       "             'around': 2104,\n",
       "             'hungarian': 18,\n",
       "             'attempted': 44,\n",
       "             'revolt': 12,\n",
       "             'soviet': 74,\n",
       "             'union': 147,\n",
       "             'seems': 894,\n",
       "             'completely': 761,\n",
       "             'negate': 24,\n",
       "             'million': 665,\n",
       "             'murdered': 49,\n",
       "             'leader': 296,\n",
       "             'hungry': 54,\n",
       "             'ukraine': 102,\n",
       "             'czechoslovakia': 11,\n",
       "             'afghanistan': 70,\n",
       "             'inside': 264,\n",
       "             'suffered': 48,\n",
       "             'communism': 45,\n",
       "             'nutrition': 9,\n",
       "             'basic': 226,\n",
       "             'freedom': 189,\n",
       "             'read': 1283,\n",
       "             'list': 661,\n",
       "             'fell': 153,\n",
       "             'meet': 243,\n",
       "             'girl': 837,\n",
       "             'dream': 240,\n",
       "             'everything': 1114,\n",
       "             'talk': 841,\n",
       "             'video_game': 127,\n",
       "             'rock': 504,\n",
       "             'climb': 84,\n",
       "             'god': 630,\n",
       "             'guitar': 228,\n",
       "             'always': 2202,\n",
       "             'learn': 383,\n",
       "             'going': 3808,\n",
       "             'motorcycle': 17,\n",
       "             'summer': 352,\n",
       "             'waiting': 270,\n",
       "             'tattoo': 47,\n",
       "             'nine': 52,\n",
       "             'month': 741,\n",
       "             'fucking': 1496,\n",
       "             'common': 419,\n",
       "             'comic': 180,\n",
       "             'blue': 422,\n",
       "             'skateboard': 9,\n",
       "             'arcade': 54,\n",
       "             'drum': 109,\n",
       "             'singing': 90,\n",
       "             'recording': 164,\n",
       "             'writing': 305,\n",
       "             'painting': 39,\n",
       "             'kayaking': 1,\n",
       "             'climbing': 42,\n",
       "             'ect': 18,\n",
       "             'recent': 235,\n",
       "             'ex': 131,\n",
       "             'claimed': 73,\n",
       "             '9': 636,\n",
       "             'dating': 45,\n",
       "             'realized': 204,\n",
       "             'involved': 262,\n",
       "             'activity': 102,\n",
       "             'suddenly': 162,\n",
       "             'time': 6500,\n",
       "             'either': 1225,\n",
       "             'worst': 576,\n",
       "             'pretty_sure': 342,\n",
       "             'realize': 402,\n",
       "             'lying': 118,\n",
       "             'case': 1183,\n",
       "             'anything': 1979,\n",
       "             'claim': 479,\n",
       "             'something': 2999,\n",
       "             'example': 960,\n",
       "             'started': 844,\n",
       "             'interested': 299,\n",
       "             'year_ago': 387,\n",
       "             'played': 1219,\n",
       "             'pac': 11,\n",
       "             'decade': 318,\n",
       "             'ago': 372,\n",
       "             'imago': 2,\n",
       "             'might': 1797,\n",
       "             'fun': 1091,\n",
       "             'actively': 90,\n",
       "             'care': 1049,\n",
       "             'hahahaha': 18,\n",
       "             'wrong': 1296,\n",
       "             'upset': 198,\n",
       "             'liverpool': 70,\n",
       "             'wonder': 453,\n",
       "             'power': 919,\n",
       "             'bill': 340,\n",
       "             'cut': 539,\n",
       "             'emission': 24,\n",
       "             'producing': 51,\n",
       "             'energy': 225,\n",
       "             'imagine': 519,\n",
       "             'higher': 555,\n",
       "             'gt': 6731,\n",
       "             'unless': 659,\n",
       "             'minority': 108,\n",
       "             'majority': 330,\n",
       "             'area': 536,\n",
       "             'white': 691,\n",
       "             'say': 4071,\n",
       "             'person': 1281,\n",
       "             'country': 1825,\n",
       "             'whose': 120,\n",
       "             'population': 359,\n",
       "             '95': 83,\n",
       "             'percent': 94,\n",
       "             'black': 680,\n",
       "             'look': 2158,\n",
       "             'burr': 8,\n",
       "             'kudi': 1,\n",
       "             'shit': 2327,\n",
       "             'video': 940,\n",
       "             'crookers': 2,\n",
       "             'remix': 37,\n",
       "             'n': 177,\n",
       "             'night': 615,\n",
       "             'http_www': 2742,\n",
       "             'youtube_com': 753,\n",
       "             'watch_v': 716,\n",
       "             'wswrepljtkc': 2,\n",
       "             'actually': 3036,\n",
       "             'album': 1101,\n",
       "             '2': 2750,\n",
       "             'kid': 1056,\n",
       "             'cudi': 7,\n",
       "             'marijuana': 116,\n",
       "             'music_video': 47,\n",
       "             'lbejc9g6ktm': 1,\n",
       "             'directed': 54,\n",
       "             'shia': 10,\n",
       "             'labeouf': 2,\n",
       "             'apparently': 326,\n",
       "             'formatting': 25,\n",
       "             'feel': 1498,\n",
       "             'strange': 151,\n",
       "             'read_article': 44,\n",
       "             'tone': 86,\n",
       "             'headed': 38,\n",
       "             'cogent': 3,\n",
       "             'fergie': 9,\n",
       "             'berba': 2,\n",
       "             'gone': 429,\n",
       "             'child': 840,\n",
       "             'deported': 11,\n",
       "             'manager': 198,\n",
       "             'speaking': 240,\n",
       "             'calmly': 7,\n",
       "             'im': 594,\n",
       "             'confused': 168,\n",
       "             'method': 142,\n",
       "             'talking': 1129,\n",
       "             'liberal': 195,\n",
       "             'politcal': 2,\n",
       "             'party': 589,\n",
       "             'steve': 118,\n",
       "             'preparing': 24,\n",
       "             'franchising': 12,\n",
       "             'matter': 1097,\n",
       "             'stir': 21,\n",
       "             'drama': 137,\n",
       "             'tl': 74,\n",
       "             'film': 780,\n",
       "             'publicity': 25,\n",
       "             'notoriety': 7,\n",
       "             'attracts': 8,\n",
       "             'investor': 19,\n",
       "             'make': 5362,\n",
       "             'money': 1729,\n",
       "             'welcome': 159,\n",
       "             'future': 514,\n",
       "             'lcs': 57,\n",
       "             'clearly': 490,\n",
       "             'shy': 16,\n",
       "             'side': 1018,\n",
       "             'expecting': 148,\n",
       "             'include': 230,\n",
       "             'proactive': 5,\n",
       "             'others': 737,\n",
       "             'uncomfortable': 40,\n",
       "             'smaller': 183,\n",
       "             'group': 1112,\n",
       "             'communicate': 54,\n",
       "             'form': 511,\n",
       "             'reach': 231,\n",
       "             'feeling': 472,\n",
       "             'equally': 81,\n",
       "             'outsider': 22,\n",
       "             'ish': 54,\n",
       "             'wish': 452,\n",
       "             'raid': 389,\n",
       "             'freeze': 41,\n",
       "             'truth': 284,\n",
       "             '100': 861,\n",
       "             'comfort': 46,\n",
       "             'zone': 277,\n",
       "             'issue': 1172,\n",
       "             'acting': 155,\n",
       "             'result': 469,\n",
       "             'sure': 2259,\n",
       "             'taken': 397,\n",
       "             'shyness': 2,\n",
       "             'trait': 65,\n",
       "             'complex': 111,\n",
       "             'content': 578,\n",
       "             'however': 780,\n",
       "             'causing': 75,\n",
       "             'solve': 72,\n",
       "             'growing': 175,\n",
       "             'pushing': 155,\n",
       "             'option': 443,\n",
       "             'try': 1631,\n",
       "             'grow': 178,\n",
       "             'reaching': 67,\n",
       "             'old': 1183,\n",
       "             'guildies': 5,\n",
       "             'focus': 289,\n",
       "             'instead': 1018,\n",
       "             'hope': 1063,\n",
       "             'consider': 428,\n",
       "             'value': 535,\n",
       "             'came': 754,\n",
       "             'along': 398,\n",
       "             'decided': 330,\n",
       "             'need': 3493,\n",
       "             'social': 301,\n",
       "             'valuable': 123,\n",
       "             'alert': 27,\n",
       "             'ton': 376,\n",
       "             'cool': 716,\n",
       "             'stopping': 76,\n",
       "             'guy': 3333,\n",
       "             'piece': 478,\n",
       "             'unique': 184,\n",
       "             'awesome': 570,\n",
       "             'together': 625,\n",
       "             'mental': 87,\n",
       "             'hangup': 4,\n",
       "             'push': 283,\n",
       "             'gamer': 17,\n",
       "             'believe': 1299,\n",
       "             'postitive': 1,\n",
       "             'negative': 203,\n",
       "             'upvote': 116,\n",
       "             'different': 1656,\n",
       "             'camp': 161,\n",
       "             'ground': 271,\n",
       "             'men': 543,\n",
       "             'lake': 52,\n",
       "             'topless': 12,\n",
       "             'program': 199,\n",
       "             'liked': 336,\n",
       "             'biatch': 1,\n",
       "             'euron': 199,\n",
       "             'effed': 2,\n",
       "             'fleet': 68,\n",
       "             '8': 766,\n",
       "             'stair': 32,\n",
       "             '65': 76,\n",
       "             'grand': 126,\n",
       "             'pointed': 64,\n",
       "             'post': 2360,\n",
       "             'france': 211,\n",
       "             'treaty': 63,\n",
       "             'stipulated': 2,\n",
       "             'declaring': 19,\n",
       "             'war': 854,\n",
       "             'perpetualhorrors': 1,\n",
       "             'post_removed': 251,\n",
       "             'updated': 67,\n",
       "             'hall_fame': 86,\n",
       "             'reddit_com': 1296,\n",
       "             'r_music': 785,\n",
       "             'wiki': 535,\n",
       "             'halloffame': 25,\n",
       "             'bot_action': 813,\n",
       "             'performed_automatically': 812,\n",
       "             'please_contact': 887,\n",
       "             'moderator_subreddit': 810,\n",
       "             'message_compose': 1126,\n",
       "             'question_concern': 838,\n",
       "             'hypocritical': 30,\n",
       "             'concerned': 104,\n",
       "             'little': 1452,\n",
       "             'puppy': 30,\n",
       "             'even': 5446,\n",
       "             'flinch': 1,\n",
       "             'evolutionary': 8,\n",
       "             'cousin': 106,\n",
       "             'slaughtered': 22,\n",
       "             'canada': 800,\n",
       "             'law': 810,\n",
       "             'irrelevant': 131,\n",
       "             'dog': 356,\n",
       "             'cow': 43,\n",
       "             'pig': 40,\n",
       "             'mammal': 9,\n",
       "             'sentient': 7,\n",
       "             'being': 34,\n",
       "             'logical': 112,\n",
       "             'sense': 448,\n",
       "             'neglecting': 7,\n",
       "             'encouraging': 51,\n",
       "             'abuse': 133,\n",
       "             'killing': 307,\n",
       "             'maintain': 81,\n",
       "             'omnivore': 4,\n",
       "             'cognitive': 20,\n",
       "             'dissonance': 13,\n",
       "             'topic': 353,\n",
       "             'overall': 307,\n",
       "             'source': 592,\n",
       "             'toi': 6,\n",
       "             'whole': 1262,\n",
       "             'tournament': 276,\n",
       "             'individual': 265,\n",
       "             'iihf': 13,\n",
       "             'site': 375,\n",
       "             'http': 1207,\n",
       "             'worldjunior2017': 1,\n",
       "             'com': 2316,\n",
       "             'en': 221,\n",
       "             '2017': 264,\n",
       "             '01': 91,\n",
       "             '05': 84,\n",
       "             'usa': 200,\n",
       "             'v': 656,\n",
       "             'statistic': 110,\n",
       "             'tab': 45,\n",
       "             '4th': 147,\n",
       "             'minute': 633,\n",
       "             'among': 242,\n",
       "             'forward': 269,\n",
       "             'semifinal': 4,\n",
       "             'final': 567,\n",
       "             'least': 1653,\n",
       "             'figured': 135,\n",
       "             'enough': 1487,\n",
       "             'confirm': 83,\n",
       "             'remembered': 64,\n",
       "             'speed': 298,\n",
       "             'subbed': 19,\n",
       "             'anyway': 550,\n",
       "             'watch': 1446,\n",
       "             'week': 868,\n",
       "             'currently': 421,\n",
       "             'middle': 350,\n",
       "             '5': 1610,\n",
       "             'trigun': 7,\n",
       "             'star': 524,\n",
       "             'darker': 26,\n",
       "             'firefly': 12,\n",
       "             'swear': 96,\n",
       "             'channel': 193,\n",
       "             'author': 103,\n",
       "             'sarcasm': 50,\n",
       "             'oblivious': 10,\n",
       "             'quote': 188,\n",
       "             'twist': 78,\n",
       "             'loop': 51,\n",
       "             'ending': 262,\n",
       "             '90': 380,\n",
       "             'cue': 16,\n",
       "             'everyone': 1613,\n",
       "             'burning': 100,\n",
       "             'theater': 136,\n",
       "             'anno': 2,\n",
       "             'away': 1146,\n",
       "             'rich': 251,\n",
       "             'made': 2458,\n",
       "             'sidebar_r': 41,\n",
       "             'microgrowery': 2,\n",
       "             'www': 67,\n",
       "             'growweedeasy': 1,\n",
       "             'seed': 147,\n",
       "             'seedfinder': 2,\n",
       "             'eu': 886,\n",
       "             'quantum': 17,\n",
       "             'led': 174,\n",
       "             'cob': 2,\n",
       "             'efficient': 67,\n",
       "             'rig': 22,\n",
       "             'hid': 14,\n",
       "             'temp': 25,\n",
       "             '70': 246,\n",
       "             'enclosure': 2,\n",
       "             'dark': 279,\n",
       "             '12': 499,\n",
       "             'hr': 121,\n",
       "             'flower': 44,\n",
       "             'cannot': 322,\n",
       "             'autoflower': 1,\n",
       "             'carbon': 24,\n",
       "             'filter': 63,\n",
       "             'seriously': 511,\n",
       "             'lebron': 330,\n",
       "             'playbook': 9,\n",
       "             'wake': 97,\n",
       "             'bus': 66,\n",
       "             'give': 1931,\n",
       "             'ball': 724,\n",
       "             'tired': 141,\n",
       "             'kyrie': 192,\n",
       "             'jr': 114,\n",
       "             'shoot': 225,\n",
       "             'phrased': 12,\n",
       "             'munchies': 16,\n",
       "             'hard': 1445,\n",
       "             'comedown': 1,\n",
       "             'every_time': 326,\n",
       "             'deserve': 179,\n",
       "             'lean': 37,\n",
       "             'number': 1083,\n",
       "             'without': 1707,\n",
       "             'reference': 261,\n",
       "             'avenger': 43,\n",
       "             'capt': 1,\n",
       "             'america': 511,\n",
       "             'successful': 176,\n",
       "             'alone': 324,\n",
       "             'moreover': 13,\n",
       "             'captain': 123,\n",
       "             'literally': 948,\n",
       "             'hijacked': 6,\n",
       "             'especially': 907,\n",
       "             'thor': 41,\n",
       "             'order': 599,\n",
       "             'initially': 65,\n",
       "             'complaining': 148,\n",
       "             'ca': 130,\n",
       "             'stuff': 1193,\n",
       "             'le': 1554,\n",
       "             'irritated': 11,\n",
       "             'rather': 886,\n",
       "             'cluttered': 4,\n",
       "             'fleshed': 18,\n",
       "             'require': 130,\n",
       "             'earlier': 216,\n",
       "             'screenplay': 5,\n",
       "             'personality': 136,\n",
       "             'clear': 538,\n",
       "             'importantly': 43,\n",
       "             'undergo': 6,\n",
       "             'character_development': 41,\n",
       "             'iron': 167,\n",
       "             'succeed': 49,\n",
       "             'standalone': 8,\n",
       "             'pretty': 1832,\n",
       "             'simple': 370,\n",
       "             'denver': 56,\n",
       "             'fan': 1799,\n",
       "             'gm': 175,\n",
       "             'watched': 469,\n",
       "             'super_bowl': 143,\n",
       "             'inexplicably': 2,\n",
       "             'circumstance': 101,\n",
       "             'sell': 361,\n",
       "             'propane': 5,\n",
       "             'accessory': 6,\n",
       "             'there': 109,\n",
       "             'stretched': 22,\n",
       "             'resolution': 40,\n",
       "             'donation': 37,\n",
       "             'notification': 24,\n",
       "             'green': 271,\n",
       "             '1995': 60,\n",
       "             '13': 300,\n",
       "             'teenager': 76,\n",
       "             'hoped': 31,\n",
       "             'gravol': 1,\n",
       "             'ensure': 72,\n",
       "             'sleep': 184,\n",
       "             'hate': 1044,\n",
       "             'highlight': 129,\n",
       "             'respect': 276,\n",
       "             'crazy': 492,\n",
       "             'celebration': 28,\n",
       "             'humble': 38,\n",
       "             'none': 339,\n",
       "             'goal': 564,\n",
       "             'looked': 455,\n",
       "             'score': 313,\n",
       "             'anyone': 1188,\n",
       "             'gretzky': 42,\n",
       "             'sid': 12,\n",
       "             'league': 1084,\n",
       "             'sc': 14,\n",
       "             'uconn': 6,\n",
       "             'last_year': 549,\n",
       "             'decent': 425,\n",
       "             'wnba': 5,\n",
       "             'dude': 802,\n",
       "             'missed': 303,\n",
       "             'chance': 887,\n",
       "             'mystery': 68,\n",
       "             'everything_else': 97,\n",
       "             'lower': 408,\n",
       "             'jump': 253,\n",
       "             'nfl': 372,\n",
       "             'rethink': 10,\n",
       "             'bit': 1260,\n",
       "             'young': 539,\n",
       "             'obviously': 589,\n",
       "             'genuine': 52,\n",
       "             'starstruck': 1,\n",
       "             'brave': 69,\n",
       "             'solid': 284,\n",
       "             'b': 462,\n",
       "             'fact': 1486,\n",
       "             'possible': 701,\n",
       "             'playoff': 582,\n",
       "             'exciting': 83,\n",
       "             'told': 552,\n",
       "             'hovering': 11,\n",
       "             '500': 186,\n",
       "             'thrilled': 16,\n",
       "             'pitching': 74,\n",
       "             'shaky': 12,\n",
       "             'horrible': 207,\n",
       "             'freddie': 26,\n",
       "             'skipped': 26,\n",
       "             'beat': 492,\n",
       "             'injury': 295,\n",
       "             'ender': 5,\n",
       "             'cf': 24,\n",
       "             'johan': 5,\n",
       "             'camargo': 4,\n",
       "             'exceeding': 5,\n",
       "             'expectation': 96,\n",
       "             'trade': 680,\n",
       "             'vet': 51,\n",
       "             'soon': 420,\n",
       "             'fair': 506,\n",
       "             'igls': 4,\n",
       "             ...})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'one',\n",
       " 'would',\n",
       " 'get',\n",
       " 'people',\n",
       " 'think',\n",
       " 'gt',\n",
       " 'time',\n",
       " 'really',\n",
       " 'know']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_count=20,window=2,size=300,sample=6e-5,alpha=0.03,min_alpha=0.0007,             negative=20,workers=cores-1,epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=50, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:01:56: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('really', 0.4021736681461334),\n",
       " ('think', 0.3312563896179199),\n",
       " ('much', 0.31276416778564453),\n",
       " ('impactful', 0.31093668937683105),\n",
       " ('actually', 0.30668026208877563),\n",
       " ('though', 0.29392924904823303),\n",
       " ('probably', 0.27342164516448975),\n",
       " ('anyone', 0.2730477452278137),\n",
       " ('would', 0.271810919046402),\n",
       " ('delusional', 0.2530396580696106)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"honestly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'honestly'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match([\"honestly\", \"person\", \"people\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50759155"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity(\"people\", 'person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:14:30: collecting all words and their counts\n",
      "INFO - 11:14:30: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 11:14:31: PROGRESS: at sentence #10000, processed 215073 words, keeping 24562 word types\n",
      "INFO - 11:14:32: PROGRESS: at sentence #20000, processed 435130 words, keeping 35421 word types\n",
      "INFO - 11:14:32: PROGRESS: at sentence #30000, processed 651529 words, keeping 43839 word types\n",
      "INFO - 11:14:33: PROGRESS: at sentence #40000, processed 874326 words, keeping 51146 word types\n",
      "INFO - 11:14:34: PROGRESS: at sentence #50000, processed 1094880 words, keeping 57292 word types\n",
      "INFO - 11:14:35: PROGRESS: at sentence #60000, processed 1311363 words, keeping 63296 word types\n",
      "INFO - 11:14:36: collected 68560 word types from a corpus of 1525906 raw words and 70000 sentences\n",
      "INFO - 11:14:36: Loading a fresh vocabulary\n",
      "INFO - 11:14:36: effective_min_count=10 retains 12459 unique words (18% of original 68560, drops 56101)\n",
      "INFO - 11:14:36: effective_min_count=10 leaves 1408349 word corpus (92% of original 1525906, drops 117557)\n",
      "INFO - 11:14:36: deleting the raw counts dictionary of 68560 items\n",
      "INFO - 11:14:36: sample=6e-05 downsamples 1202 most-common words\n",
      "INFO - 11:14:36: downsampling leaves estimated 875919 word corpus (62.2% of prior 1408349)\n",
      "INFO - 11:14:37: estimated required memory for 12459 words and 300 dimensions: 36131100 bytes\n",
      "INFO - 11:14:37: resetting layer weights\n",
      "INFO - 11:14:40: training model with 7 workers on 12459 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.16 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:14:41: EPOCH 1 - PROGRESS: at 12.60% examples, 107162 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:42: EPOCH 1 - PROGRESS: at 26.25% examples, 111446 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:43: EPOCH 1 - PROGRESS: at 39.86% examples, 111870 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:44: EPOCH 1 - PROGRESS: at 53.44% examples, 112697 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:45: EPOCH 1 - PROGRESS: at 66.17% examples, 112899 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:46: EPOCH 1 - PROGRESS: at 79.89% examples, 113266 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:47: EPOCH 1 - PROGRESS: at 94.08% examples, 113838 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:47: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:14:47: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:14:47: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:14:47: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:14:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:14:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:14:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:14:47: EPOCH - 1 : training on 1525906 raw words (876221 effective words) took 7.7s, 114222 effective words/s\n",
      "INFO - 11:14:48: EPOCH 2 - PROGRESS: at 12.60% examples, 107755 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:49: EPOCH 2 - PROGRESS: at 26.25% examples, 112759 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:50: EPOCH 2 - PROGRESS: at 38.65% examples, 110748 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:51: EPOCH 2 - PROGRESS: at 52.04% examples, 112104 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:52: EPOCH 2 - PROGRESS: at 65.56% examples, 113105 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:53: EPOCH 2 - PROGRESS: at 79.30% examples, 113534 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:54: EPOCH 2 - PROGRESS: at 93.39% examples, 113953 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:55: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:14:55: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:14:55: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:14:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:14:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:14:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:14:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:14:55: EPOCH - 2 : training on 1525906 raw words (875467 effective words) took 7.6s, 114456 effective words/s\n",
      "INFO - 11:14:56: EPOCH 3 - PROGRESS: at 12.60% examples, 107556 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:57: EPOCH 3 - PROGRESS: at 26.25% examples, 113038 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:58: EPOCH 3 - PROGRESS: at 39.86% examples, 113633 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:14:59: EPOCH 3 - PROGRESS: at 52.04% examples, 112218 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:00: EPOCH 3 - PROGRESS: at 64.92% examples, 111988 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:01: EPOCH 3 - PROGRESS: at 78.67% examples, 112626 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:02: EPOCH 3 - PROGRESS: at 92.09% examples, 112733 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:03: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:15:03: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:15:03: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:15:03: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:15:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:15:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:15:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:15:03: EPOCH - 3 : training on 1525906 raw words (875965 effective words) took 7.7s, 113312 effective words/s\n",
      "INFO - 11:15:04: EPOCH 4 - PROGRESS: at 11.33% examples, 94182 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:05: EPOCH 4 - PROGRESS: at 22.26% examples, 94387 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:06: EPOCH 4 - PROGRESS: at 33.34% examples, 93541 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:07: EPOCH 4 - PROGRESS: at 44.43% examples, 94310 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:08: EPOCH 4 - PROGRESS: at 55.23% examples, 94862 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:09: EPOCH 4 - PROGRESS: at 66.17% examples, 95029 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:10: EPOCH 4 - PROGRESS: at 77.93% examples, 95228 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 11:15:11: EPOCH 4 - PROGRESS: at 89.39% examples, 95267 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:12: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:15:12: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:15:12: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:15:12: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:15:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:15:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:15:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:15:12: EPOCH - 4 : training on 1525906 raw words (875496 effective words) took 9.0s, 96749 effective words/s\n",
      "INFO - 11:15:13: EPOCH 5 - PROGRESS: at 13.33% examples, 109821 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:14: EPOCH 5 - PROGRESS: at 26.86% examples, 113866 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:15: EPOCH 5 - PROGRESS: at 40.55% examples, 114833 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:16: EPOCH 5 - PROGRESS: at 54.05% examples, 114948 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:17: EPOCH 5 - PROGRESS: at 66.82% examples, 114062 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:18: EPOCH 5 - PROGRESS: at 79.89% examples, 114111 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:19: EPOCH 5 - PROGRESS: at 94.08% examples, 114295 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:15:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:15:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:15:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:15:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:15:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:15:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:15:19: EPOCH - 5 : training on 1525906 raw words (875924 effective words) took 7.6s, 114596 effective words/s\n",
      "INFO - 11:15:20: EPOCH 6 - PROGRESS: at 12.60% examples, 107757 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:21: EPOCH 6 - PROGRESS: at 26.25% examples, 112216 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:22: EPOCH 6 - PROGRESS: at 39.86% examples, 113192 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:24: EPOCH 6 - PROGRESS: at 52.77% examples, 113045 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:25: EPOCH 6 - PROGRESS: at 65.56% examples, 113000 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:26: EPOCH 6 - PROGRESS: at 79.30% examples, 113400 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:27: EPOCH 6 - PROGRESS: at 93.39% examples, 113298 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:15:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:15:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:15:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:15:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:15:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:15:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:15:27: EPOCH - 6 : training on 1525906 raw words (876030 effective words) took 7.7s, 113998 effective words/s\n",
      "INFO - 11:15:28: EPOCH 7 - PROGRESS: at 12.60% examples, 108038 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:29: EPOCH 7 - PROGRESS: at 26.25% examples, 113528 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:15:30: EPOCH 7 - PROGRESS: at 39.86% examples, 115226 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:31: EPOCH 7 - PROGRESS: at 52.77% examples, 114985 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:32: EPOCH 7 - PROGRESS: at 66.17% examples, 115310 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:33: EPOCH 7 - PROGRESS: at 79.89% examples, 115460 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:34: EPOCH 7 - PROGRESS: at 93.39% examples, 115292 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:35: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:15:35: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:15:35: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:15:35: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:15:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:15:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:15:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:15:35: EPOCH - 7 : training on 1525906 raw words (876496 effective words) took 7.6s, 115519 effective words/s\n",
      "INFO - 11:15:36: EPOCH 8 - PROGRESS: at 13.33% examples, 113467 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:37: EPOCH 8 - PROGRESS: at 26.86% examples, 114598 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:38: EPOCH 8 - PROGRESS: at 40.55% examples, 115959 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:39: EPOCH 8 - PROGRESS: at 53.44% examples, 115254 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:40: EPOCH 8 - PROGRESS: at 66.17% examples, 115002 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:41: EPOCH 8 - PROGRESS: at 79.30% examples, 114665 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:42: EPOCH 8 - PROGRESS: at 92.75% examples, 114435 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:42: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:15:42: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:15:42: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:15:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:15:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:15:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:15:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:15:42: EPOCH - 8 : training on 1525906 raw words (875581 effective words) took 7.6s, 114868 effective words/s\n",
      "INFO - 11:15:43: EPOCH 9 - PROGRESS: at 13.33% examples, 109969 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:44: EPOCH 9 - PROGRESS: at 26.86% examples, 114749 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:45: EPOCH 9 - PROGRESS: at 40.55% examples, 114846 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:46: EPOCH 9 - PROGRESS: at 54.05% examples, 115676 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:47: EPOCH 9 - PROGRESS: at 67.47% examples, 115732 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:48: EPOCH 9 - PROGRESS: at 80.52% examples, 115536 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:49: EPOCH 9 - PROGRESS: at 94.08% examples, 115256 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:50: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:15:50: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:15:50: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:15:50: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:15:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:15:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:15:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:15:50: EPOCH - 9 : training on 1525906 raw words (875683 effective words) took 7.6s, 115329 effective words/s\n",
      "INFO - 11:15:51: EPOCH 10 - PROGRESS: at 12.60% examples, 107861 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:52: EPOCH 10 - PROGRESS: at 26.25% examples, 111604 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:53: EPOCH 10 - PROGRESS: at 39.86% examples, 112298 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:54: EPOCH 10 - PROGRESS: at 53.44% examples, 113278 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:55: EPOCH 10 - PROGRESS: at 66.17% examples, 113565 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:56: EPOCH 10 - PROGRESS: at 79.89% examples, 114455 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:57: EPOCH 10 - PROGRESS: at 93.39% examples, 114310 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:15:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:15:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:15:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:15:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:15:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:15:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:15:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:15:58: EPOCH - 10 : training on 1525906 raw words (876314 effective words) took 7.6s, 114760 effective words/s\n",
      "INFO - 11:15:59: EPOCH 11 - PROGRESS: at 13.33% examples, 110133 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:00: EPOCH 11 - PROGRESS: at 26.86% examples, 113615 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:01: EPOCH 11 - PROGRESS: at 39.86% examples, 113257 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:02: EPOCH 11 - PROGRESS: at 53.44% examples, 114161 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:03: EPOCH 11 - PROGRESS: at 66.17% examples, 114233 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:04: EPOCH 11 - PROGRESS: at 79.30% examples, 113971 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:05: EPOCH 11 - PROGRESS: at 93.39% examples, 114373 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:05: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:16:05: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:16:05: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:16:05: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:16:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:16:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:16:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:16:05: EPOCH - 11 : training on 1525906 raw words (876768 effective words) took 7.7s, 114598 effective words/s\n",
      "INFO - 11:16:06: EPOCH 12 - PROGRESS: at 12.60% examples, 108249 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:07: EPOCH 12 - PROGRESS: at 26.25% examples, 112928 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:08: EPOCH 12 - PROGRESS: at 39.23% examples, 112033 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:09: EPOCH 12 - PROGRESS: at 52.77% examples, 113470 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:10: EPOCH 12 - PROGRESS: at 66.17% examples, 113985 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:11: EPOCH 12 - PROGRESS: at 79.30% examples, 113551 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:12: EPOCH 12 - PROGRESS: at 93.39% examples, 113912 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:13: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:16:13: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:16:13: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:16:13: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:16:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:16:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:16:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:16:13: EPOCH - 12 : training on 1525906 raw words (875336 effective words) took 7.6s, 114524 effective words/s\n",
      "INFO - 11:16:14: EPOCH 13 - PROGRESS: at 12.00% examples, 101139 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:15: EPOCH 13 - PROGRESS: at 24.93% examples, 107680 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:16: EPOCH 13 - PROGRESS: at 38.65% examples, 110436 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:17: EPOCH 13 - PROGRESS: at 52.04% examples, 111142 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:18: EPOCH 13 - PROGRESS: at 65.56% examples, 112255 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:16:19: EPOCH 13 - PROGRESS: at 79.30% examples, 112766 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:20: EPOCH 13 - PROGRESS: at 92.75% examples, 112704 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:16:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:16:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:16:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:16:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:16:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:16:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:16:21: EPOCH - 13 : training on 1525906 raw words (876150 effective words) took 7.7s, 113443 effective words/s\n",
      "INFO - 11:16:22: EPOCH 14 - PROGRESS: at 7.88% examples, 65557 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:23: EPOCH 14 - PROGRESS: at 17.10% examples, 72966 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:24: EPOCH 14 - PROGRESS: at 29.36% examples, 83649 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:25: EPOCH 14 - PROGRESS: at 43.22% examples, 91481 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:26: EPOCH 14 - PROGRESS: at 56.53% examples, 96272 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:27: EPOCH 14 - PROGRESS: at 68.85% examples, 97422 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:28: EPOCH 14 - PROGRESS: at 84.50% examples, 102744 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:29: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:16:29: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:16:29: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:16:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:16:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:16:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:16:29: EPOCH 14 - PROGRESS: at 100.00% examples, 105785 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:16:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:16:29: EPOCH - 14 : training on 1525906 raw words (875720 effective words) took 8.3s, 105766 effective words/s\n",
      "INFO - 11:16:30: EPOCH 15 - PROGRESS: at 10.60% examples, 89647 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:31: EPOCH 15 - PROGRESS: at 22.92% examples, 98497 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:32: EPOCH 15 - PROGRESS: at 39.23% examples, 112330 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:33: EPOCH 15 - PROGRESS: at 54.05% examples, 116205 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:34: EPOCH 15 - PROGRESS: at 70.18% examples, 120791 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:35: EPOCH 15 - PROGRESS: at 88.02% examples, 125762 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:36: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:16:36: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:16:36: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:16:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:16:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:16:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:16:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:16:36: EPOCH - 15 : training on 1525906 raw words (875191 effective words) took 6.9s, 127490 effective words/s\n",
      "INFO - 11:16:37: EPOCH 16 - PROGRESS: at 15.73% examples, 132399 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:38: EPOCH 16 - PROGRESS: at 31.30% examples, 132946 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:39: EPOCH 16 - PROGRESS: at 48.32% examples, 137242 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:40: EPOCH 16 - PROGRESS: at 64.92% examples, 138473 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:41: EPOCH 16 - PROGRESS: at 81.16% examples, 139236 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:42: EPOCH 16 - PROGRESS: at 96.79% examples, 137323 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:42: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:16:42: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:16:42: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:16:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:16:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:16:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:16:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:16:42: EPOCH - 16 : training on 1525906 raw words (875770 effective words) took 6.4s, 137519 effective words/s\n",
      "INFO - 11:16:43: EPOCH 17 - PROGRESS: at 10.60% examples, 88237 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:44: EPOCH 17 - PROGRESS: at 23.57% examples, 100905 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:45: EPOCH 17 - PROGRESS: at 37.97% examples, 108532 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:46: EPOCH 17 - PROGRESS: at 50.74% examples, 109665 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:47: EPOCH 17 - PROGRESS: at 67.47% examples, 117053 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:48: EPOCH 17 - PROGRESS: at 84.50% examples, 121356 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:49: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:16:49: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:16:49: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:16:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:16:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:16:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:16:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:16:49: EPOCH - 17 : training on 1525906 raw words (876443 effective words) took 7.0s, 124454 effective words/s\n",
      "INFO - 11:16:50: EPOCH 18 - PROGRESS: at 16.36% examples, 137121 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:51: EPOCH 18 - PROGRESS: at 33.34% examples, 141673 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:52: EPOCH 18 - PROGRESS: at 46.99% examples, 133713 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:53: EPOCH 18 - PROGRESS: at 58.59% examples, 125688 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:54: EPOCH 18 - PROGRESS: at 68.18% examples, 117081 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:55: EPOCH 18 - PROGRESS: at 76.16% examples, 107735 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:57: EPOCH 18 - PROGRESS: at 83.79% examples, 101291 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:58: EPOCH 18 - PROGRESS: at 90.68% examples, 95651 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:16:59: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:16:59: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:16:59: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:16:59: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:16:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:16:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:16:59: EPOCH 18 - PROGRESS: at 100.00% examples, 93551 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:16:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:16:59: EPOCH - 18 : training on 1525906 raw words (875854 effective words) took 9.4s, 93533 effective words/s\n",
      "INFO - 11:17:00: EPOCH 19 - PROGRESS: at 11.33% examples, 94359 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:01: EPOCH 19 - PROGRESS: at 18.31% examples, 78728 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:02: EPOCH 19 - PROGRESS: at 28.81% examples, 81883 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:03: EPOCH 19 - PROGRESS: at 40.55% examples, 86568 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:04: EPOCH 19 - PROGRESS: at 53.44% examples, 91354 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:05: EPOCH 19 - PROGRESS: at 61.83% examples, 87209 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:06: EPOCH 19 - PROGRESS: at 74.82% examples, 90290 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:07: EPOCH 19 - PROGRESS: at 88.02% examples, 92990 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:17:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:17:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:17:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:17:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:17:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:17:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:17:08: EPOCH 19 - PROGRESS: at 100.00% examples, 93753 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:17:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:17:08: EPOCH - 19 : training on 1525906 raw words (876701 effective words) took 9.4s, 93736 effective words/s\n",
      "INFO - 11:17:09: EPOCH 20 - PROGRESS: at 7.30% examples, 60495 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:10: EPOCH 20 - PROGRESS: at 19.67% examples, 83190 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 11:17:11: EPOCH 20 - PROGRESS: at 33.34% examples, 94919 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:12: EPOCH 20 - PROGRESS: at 45.76% examples, 97634 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:13: EPOCH 20 - PROGRESS: at 54.05% examples, 92841 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:14: EPOCH 20 - PROGRESS: at 66.17% examples, 95364 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:15: EPOCH 20 - PROGRESS: at 79.30% examples, 97674 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:16: EPOCH 20 - PROGRESS: at 91.40% examples, 98076 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:17:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:17:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:17:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:17:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:17:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:17:17: EPOCH 20 - PROGRESS: at 100.00% examples, 95487 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:17:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:17:17: EPOCH - 20 : training on 1525906 raw words (875283 effective words) took 9.2s, 95473 effective words/s\n",
      "INFO - 11:17:18: EPOCH 21 - PROGRESS: at 13.33% examples, 109850 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:19: EPOCH 21 - PROGRESS: at 26.86% examples, 113014 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:20: EPOCH 21 - PROGRESS: at 38.65% examples, 106987 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:21: EPOCH 21 - PROGRESS: at 47.64% examples, 100158 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:22: EPOCH 21 - PROGRESS: at 60.50% examples, 102205 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:23: EPOCH 21 - PROGRESS: at 73.49% examples, 104018 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:24: EPOCH 21 - PROGRESS: at 83.79% examples, 101483 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:25: EPOCH 21 - PROGRESS: at 94.08% examples, 99445 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:26: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:17:26: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:17:26: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:17:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:17:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:17:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:17:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:17:26: EPOCH - 21 : training on 1525906 raw words (876737 effective words) took 8.9s, 98675 effective words/s\n",
      "INFO - 11:17:27: EPOCH 22 - PROGRESS: at 7.30% examples, 59239 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:28: EPOCH 22 - PROGRESS: at 14.54% examples, 58461 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:29: EPOCH 22 - PROGRESS: at 19.67% examples, 52702 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:30: EPOCH 22 - PROGRESS: at 25.56% examples, 51869 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:31: EPOCH 22 - PROGRESS: at 31.30% examples, 51603 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:32: EPOCH 22 - PROGRESS: at 39.23% examples, 53894 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:33: EPOCH 22 - PROGRESS: at 45.76% examples, 54287 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:35: EPOCH 22 - PROGRESS: at 52.04% examples, 54068 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:36: EPOCH 22 - PROGRESS: at 57.91% examples, 53779 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:37: EPOCH 22 - PROGRESS: at 64.33% examples, 53763 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:38: EPOCH 22 - PROGRESS: at 70.18% examples, 53327 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:39: EPOCH 22 - PROGRESS: at 76.16% examples, 53002 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:40: EPOCH 22 - PROGRESS: at 81.76% examples, 52888 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:41: EPOCH 22 - PROGRESS: at 88.02% examples, 52767 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:42: EPOCH 22 - PROGRESS: at 96.08% examples, 53741 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:42: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:17:42: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:17:42: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:17:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:17:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:17:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:17:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:17:42: EPOCH - 22 : training on 1525906 raw words (875861 effective words) took 15.9s, 55124 effective words/s\n",
      "INFO - 11:17:43: EPOCH 23 - PROGRESS: at 12.00% examples, 102012 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:44: EPOCH 23 - PROGRESS: at 24.27% examples, 104896 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:45: EPOCH 23 - PROGRESS: at 40.55% examples, 116448 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:46: EPOCH 23 - PROGRESS: at 53.44% examples, 115177 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:47: EPOCH 23 - PROGRESS: at 65.56% examples, 113650 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:48: EPOCH 23 - PROGRESS: at 81.76% examples, 117836 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:49: EPOCH 23 - PROGRESS: at 96.08% examples, 117161 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:49: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:17:49: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:17:49: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:17:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:17:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:17:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:17:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:17:49: EPOCH - 23 : training on 1525906 raw words (875358 effective words) took 7.4s, 117988 effective words/s\n",
      "INFO - 11:17:50: EPOCH 24 - PROGRESS: at 12.60% examples, 103863 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:52: EPOCH 24 - PROGRESS: at 25.56% examples, 108204 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:53: EPOCH 24 - PROGRESS: at 41.77% examples, 118061 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:54: EPOCH 24 - PROGRESS: at 54.62% examples, 116083 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:55: EPOCH 24 - PROGRESS: at 67.47% examples, 115106 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:56: EPOCH 24 - PROGRESS: at 83.79% examples, 119017 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:17:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:17:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:17:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:17:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:17:57: EPOCH 24 - PROGRESS: at 99.45% examples, 120542 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 11:17:57: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:17:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:17:57: EPOCH - 24 : training on 1525906 raw words (876143 effective words) took 7.3s, 120485 effective words/s\n",
      "INFO - 11:17:58: EPOCH 25 - PROGRESS: at 9.26% examples, 75711 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:17:59: EPOCH 25 - PROGRESS: at 17.10% examples, 70217 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:00: EPOCH 25 - PROGRESS: at 25.56% examples, 70722 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:01: EPOCH 25 - PROGRESS: at 41.17% examples, 85227 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:02: EPOCH 25 - PROGRESS: at 52.04% examples, 87092 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:03: EPOCH 25 - PROGRESS: at 61.16% examples, 85616 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:04: EPOCH 25 - PROGRESS: at 72.10% examples, 86699 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:05: EPOCH 25 - PROGRESS: at 85.92% examples, 90170 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:06: EPOCH 25 - PROGRESS: at 98.02% examples, 91333 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:06: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:18:06: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:18:06: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:18:06: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:18:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:18:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:18:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:18:06: EPOCH - 25 : training on 1525906 raw words (875138 effective words) took 9.6s, 91352 effective words/s\n",
      "INFO - 11:18:07: EPOCH 26 - PROGRESS: at 12.00% examples, 98036 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:08: EPOCH 26 - PROGRESS: at 25.56% examples, 106017 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:09: EPOCH 26 - PROGRESS: at 37.28% examples, 104313 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:10: EPOCH 26 - PROGRESS: at 49.55% examples, 104825 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:11: EPOCH 26 - PROGRESS: at 63.02% examples, 107825 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:12: EPOCH 26 - PROGRESS: at 74.20% examples, 105683 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:13: EPOCH 26 - PROGRESS: at 85.92% examples, 105085 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:14: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:18:14: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:18:14: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:18:14: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:18:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:18:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:18:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:18:14: EPOCH - 26 : training on 1525906 raw words (875751 effective words) took 8.1s, 107816 effective words/s\n",
      "INFO - 11:18:16: EPOCH 27 - PROGRESS: at 12.00% examples, 96519 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:17: EPOCH 27 - PROGRESS: at 24.27% examples, 99589 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:18: EPOCH 27 - PROGRESS: at 39.86% examples, 110668 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:19: EPOCH 27 - PROGRESS: at 54.05% examples, 114263 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:20: EPOCH 27 - PROGRESS: at 66.17% examples, 112413 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:21: EPOCH 27 - PROGRESS: at 77.93% examples, 110872 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:22: EPOCH 27 - PROGRESS: at 94.08% examples, 113721 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:18:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:18:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:18:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:18:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:18:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:18:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:18:22: EPOCH - 27 : training on 1525906 raw words (876135 effective words) took 7.6s, 114938 effective words/s\n",
      "INFO - 11:18:23: EPOCH 28 - PROGRESS: at 12.60% examples, 107192 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:24: EPOCH 28 - PROGRESS: at 24.93% examples, 105588 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:25: EPOCH 28 - PROGRESS: at 40.55% examples, 115470 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:26: EPOCH 28 - PROGRESS: at 55.87% examples, 120633 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:27: EPOCH 28 - PROGRESS: at 71.43% examples, 123980 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:28: EPOCH 28 - PROGRESS: at 83.79% examples, 121407 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:29: EPOCH 28 - PROGRESS: at 91.40% examples, 113128 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:30: EPOCH 28 - PROGRESS: at 98.02% examples, 105979 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:30: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:18:30: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:18:30: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:18:30: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:18:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:18:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:18:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:18:30: EPOCH - 28 : training on 1525906 raw words (876189 effective words) took 8.3s, 105578 effective words/s\n",
      "INFO - 11:18:31: EPOCH 29 - PROGRESS: at 12.00% examples, 94477 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:33: EPOCH 29 - PROGRESS: at 21.05% examples, 86096 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:34: EPOCH 29 - PROGRESS: at 30.63% examples, 85046 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:35: EPOCH 29 - PROGRESS: at 39.86% examples, 82778 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:36: EPOCH 29 - PROGRESS: at 48.92% examples, 81383 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:37: EPOCH 29 - PROGRESS: at 57.21% examples, 79261 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:38: EPOCH 29 - PROGRESS: at 68.18% examples, 81236 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:39: EPOCH 29 - PROGRESS: at 81.76% examples, 85472 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:40: EPOCH 29 - PROGRESS: at 94.76% examples, 87850 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:40: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:18:40: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:18:40: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:18:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:18:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:18:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:18:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:18:40: EPOCH - 29 : training on 1525906 raw words (875985 effective words) took 9.8s, 89131 effective words/s\n",
      "INFO - 11:18:41: EPOCH 30 - PROGRESS: at 15.73% examples, 134436 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:42: EPOCH 30 - PROGRESS: at 31.98% examples, 136861 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:43: EPOCH 30 - PROGRESS: at 47.64% examples, 136853 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:44: EPOCH 30 - PROGRESS: at 63.68% examples, 137246 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:45: EPOCH 30 - PROGRESS: at 79.89% examples, 137628 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:46: EPOCH 30 - PROGRESS: at 96.79% examples, 137618 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:47: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:18:47: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:18:47: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:18:47: worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:18:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:18:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:18:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:18:47: EPOCH - 30 : training on 1525906 raw words (875961 effective words) took 6.3s, 138056 effective words/s\n",
      "INFO - 11:18:48: EPOCH 31 - PROGRESS: at 16.36% examples, 142938 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:49: EPOCH 31 - PROGRESS: at 34.03% examples, 146766 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:50: EPOCH 31 - PROGRESS: at 50.74% examples, 146772 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:51: EPOCH 31 - PROGRESS: at 68.18% examples, 147878 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:52: EPOCH 31 - PROGRESS: at 85.92% examples, 148996 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:52: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:18:52: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:18:52: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:18:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:18:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:18:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:18:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:18:53: EPOCH - 31 : training on 1525906 raw words (876922 effective words) took 5.9s, 149467 effective words/s\n",
      "INFO - 11:18:54: EPOCH 32 - PROGRESS: at 17.10% examples, 148459 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:55: EPOCH 32 - PROGRESS: at 34.70% examples, 147942 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:56: EPOCH 32 - PROGRESS: at 52.04% examples, 149282 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:57: EPOCH 32 - PROGRESS: at 69.49% examples, 148925 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:58: EPOCH 32 - PROGRESS: at 86.63% examples, 148743 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:18:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:18:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:18:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:18:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:18:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:18:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:18:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:18:58: EPOCH - 32 : training on 1525906 raw words (875292 effective words) took 5.9s, 148126 effective words/s\n",
      "INFO - 11:18:59: EPOCH 33 - PROGRESS: at 15.73% examples, 133969 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:00: EPOCH 33 - PROGRESS: at 31.98% examples, 137675 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:01: EPOCH 33 - PROGRESS: at 48.32% examples, 139044 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:03: EPOCH 33 - PROGRESS: at 64.92% examples, 140404 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:04: EPOCH 33 - PROGRESS: at 81.16% examples, 140723 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:05: EPOCH 33 - PROGRESS: at 98.69% examples, 141539 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:05: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:19:05: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:19:05: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:19:05: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:19:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:19:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:19:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:19:05: EPOCH - 33 : training on 1525906 raw words (876477 effective words) took 6.2s, 141581 effective words/s\n",
      "INFO - 11:19:06: EPOCH 34 - PROGRESS: at 16.36% examples, 140008 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:07: EPOCH 34 - PROGRESS: at 34.03% examples, 145093 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:08: EPOCH 34 - PROGRESS: at 52.04% examples, 148622 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:09: EPOCH 34 - PROGRESS: at 69.49% examples, 150119 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:10: EPOCH 34 - PROGRESS: at 87.29% examples, 150796 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:10: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:19:10: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:19:10: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:19:10: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:19:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:19:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:19:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:19:10: EPOCH - 34 : training on 1525906 raw words (875984 effective words) took 5.8s, 150691 effective words/s\n",
      "INFO - 11:19:11: EPOCH 35 - PROGRESS: at 17.10% examples, 148389 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:12: EPOCH 35 - PROGRESS: at 34.70% examples, 150140 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:14: EPOCH 35 - PROGRESS: at 52.04% examples, 150533 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:15: EPOCH 35 - PROGRESS: at 70.18% examples, 151901 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:16: EPOCH 35 - PROGRESS: at 88.02% examples, 151267 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:19:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:19:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:19:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:19:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:19:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:19:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:19:16: EPOCH - 35 : training on 1525906 raw words (875932 effective words) took 5.8s, 151266 effective words/s\n",
      "INFO - 11:19:17: EPOCH 36 - PROGRESS: at 17.10% examples, 144655 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:18: EPOCH 36 - PROGRESS: at 34.70% examples, 146691 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:19: EPOCH 36 - PROGRESS: at 52.04% examples, 148639 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:20: EPOCH 36 - PROGRESS: at 69.49% examples, 149645 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:21: EPOCH 36 - PROGRESS: at 86.63% examples, 149315 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:19:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:19:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:19:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:19:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:19:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:19:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:19:22: EPOCH - 36 : training on 1525906 raw words (875188 effective words) took 5.8s, 150149 effective words/s\n",
      "INFO - 11:19:23: EPOCH 37 - PROGRESS: at 16.36% examples, 139428 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:24: EPOCH 37 - PROGRESS: at 34.03% examples, 144785 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:25: EPOCH 37 - PROGRESS: at 51.40% examples, 147104 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:26: EPOCH 37 - PROGRESS: at 68.85% examples, 148367 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:27: EPOCH 37 - PROGRESS: at 87.29% examples, 149641 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:28: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:19:28: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:19:28: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:19:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:19:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:19:28: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:19:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:19:28: EPOCH - 37 : training on 1525906 raw words (875467 effective words) took 5.8s, 150263 effective words/s\n",
      "INFO - 11:19:29: EPOCH 38 - PROGRESS: at 16.36% examples, 142193 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:30: EPOCH 38 - PROGRESS: at 34.03% examples, 147151 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:31: EPOCH 38 - PROGRESS: at 51.40% examples, 149093 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:32: EPOCH 38 - PROGRESS: at 68.18% examples, 149163 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:33: EPOCH 38 - PROGRESS: at 85.92% examples, 149700 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:34: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:19:34: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:19:34: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:19:34: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:19:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:19:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:19:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:19:34: EPOCH - 38 : training on 1525906 raw words (876228 effective words) took 5.8s, 150125 effective words/s\n",
      "INFO - 11:19:35: EPOCH 39 - PROGRESS: at 17.10% examples, 148161 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:36: EPOCH 39 - PROGRESS: at 35.34% examples, 150704 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:37: EPOCH 39 - PROGRESS: at 51.40% examples, 146893 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:38: EPOCH 39 - PROGRESS: at 68.85% examples, 148340 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:39: EPOCH 39 - PROGRESS: at 85.92% examples, 148413 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:40: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:19:40: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:19:40: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:19:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:19:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:19:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:19:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:19:40: EPOCH - 39 : training on 1525906 raw words (875954 effective words) took 5.9s, 148945 effective words/s\n",
      "INFO - 11:19:41: EPOCH 40 - PROGRESS: at 16.36% examples, 143262 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:42: EPOCH 40 - PROGRESS: at 34.03% examples, 148186 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:43: EPOCH 40 - PROGRESS: at 51.40% examples, 149171 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:44: EPOCH 40 - PROGRESS: at 68.85% examples, 149903 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:45: EPOCH 40 - PROGRESS: at 85.92% examples, 149255 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:19:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:19:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:19:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:19:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:19:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:19:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:19:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:19:46: EPOCH - 40 : training on 1525906 raw words (875666 effective words) took 5.8s, 149973 effective words/s\n",
      "INFO - 11:19:46: training on a 61036240 raw words (35036761 effective words) took 305.9s, 114534 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 5.1 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=10,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=40, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:20:01: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('think', 0.3799281120300293),\n",
       " ('really', 0.345345139503479),\n",
       " ('actually', 0.2863292694091797),\n",
       " ('impactful', 0.28042757511138916),\n",
       " ('would', 0.28009873628616333),\n",
       " ('passionate', 0.27521342039108276),\n",
       " ('though', 0.27376729249954224),\n",
       " ('happen', 0.2671111822128296),\n",
       " ('unbiased', 0.25786978006362915),\n",
       " ('moronic', 0.2577594220638275)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"honestly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40952381"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity(\"people\", 'person')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:20:41: collecting all words and their counts\n",
      "INFO - 18:20:41: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:20:42: PROGRESS: at sentence #10000, processed 215073 words, keeping 24562 word types\n",
      "INFO - 18:20:42: PROGRESS: at sentence #20000, processed 435130 words, keeping 35421 word types\n",
      "INFO - 18:20:43: PROGRESS: at sentence #30000, processed 651529 words, keeping 43839 word types\n",
      "INFO - 18:20:43: PROGRESS: at sentence #40000, processed 874326 words, keeping 51146 word types\n",
      "INFO - 18:20:44: PROGRESS: at sentence #50000, processed 1094880 words, keeping 57292 word types\n",
      "INFO - 18:20:45: PROGRESS: at sentence #60000, processed 1311363 words, keeping 63296 word types\n",
      "INFO - 18:20:45: collected 68560 word types from a corpus of 1525906 raw words and 70000 sentences\n",
      "INFO - 18:20:45: Loading a fresh vocabulary\n",
      "INFO - 18:20:45: effective_min_count=10 retains 12459 unique words (18% of original 68560, drops 56101)\n",
      "INFO - 18:20:45: effective_min_count=10 leaves 1408349 word corpus (92% of original 1525906, drops 117557)\n",
      "INFO - 18:20:45: deleting the raw counts dictionary of 68560 items\n",
      "INFO - 18:20:45: sample=0.001 downsamples 30 most-common words\n",
      "INFO - 18:20:45: downsampling leaves estimated 1361231 word corpus (96.7% of prior 1408349)\n",
      "INFO - 18:20:45: estimated required memory for 12459 words and 100 dimensions: 16196700 bytes\n",
      "INFO - 18:20:45: resetting layer weights\n",
      "INFO - 18:20:48: training model with 2 workers on 12459 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 18:20:49: EPOCH 1 - PROGRESS: at 15.73% examples, 206847 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:20:50: EPOCH 1 - PROGRESS: at 30.63% examples, 204908 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:20:51: EPOCH 1 - PROGRESS: at 48.32% examples, 213780 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:20:52: EPOCH 1 - PROGRESS: at 64.92% examples, 217254 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:20:53: EPOCH 1 - PROGRESS: at 81.16% examples, 215644 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:20:54: EPOCH 1 - PROGRESS: at 92.09% examples, 202413 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:20:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:20:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:20:55: EPOCH - 1 : training on 1525906 raw words (1361172 effective words) took 6.8s, 199529 effective words/s\n",
      "INFO - 18:20:56: EPOCH 2 - PROGRESS: at 17.66% examples, 234834 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:20:57: EPOCH 2 - PROGRESS: at 34.70% examples, 231519 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:20:58: EPOCH 2 - PROGRESS: at 51.40% examples, 228515 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:20:59: EPOCH 2 - PROGRESS: at 69.49% examples, 232448 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:00: EPOCH 2 - PROGRESS: at 88.02% examples, 234714 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:01: EPOCH - 2 : training on 1525906 raw words (1361321 effective words) took 5.8s, 234127 effective words/s\n",
      "INFO - 18:21:02: EPOCH 3 - PROGRESS: at 17.66% examples, 240206 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:03: EPOCH 3 - PROGRESS: at 34.70% examples, 234336 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:04: EPOCH 3 - PROGRESS: at 52.77% examples, 236117 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:05: EPOCH 3 - PROGRESS: at 71.43% examples, 240277 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:06: EPOCH 3 - PROGRESS: at 90.06% examples, 240106 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:06: EPOCH - 3 : training on 1525906 raw words (1361008 effective words) took 5.7s, 240846 effective words/s\n",
      "INFO - 18:21:07: EPOCH 4 - PROGRESS: at 17.66% examples, 237246 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:08: EPOCH 4 - PROGRESS: at 36.03% examples, 241807 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:09: EPOCH 4 - PROGRESS: at 53.44% examples, 239911 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:10: EPOCH 4 - PROGRESS: at 70.77% examples, 239956 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:11: EPOCH 4 - PROGRESS: at 89.39% examples, 241159 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:12: EPOCH - 4 : training on 1525906 raw words (1361257 effective words) took 5.6s, 241004 effective words/s\n",
      "INFO - 18:21:13: EPOCH 5 - PROGRESS: at 14.54% examples, 192722 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:14: EPOCH 5 - PROGRESS: at 30.63% examples, 205072 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:15: EPOCH 5 - PROGRESS: at 47.64% examples, 210777 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:16: EPOCH 5 - PROGRESS: at 64.33% examples, 213919 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:17: EPOCH 5 - PROGRESS: at 81.76% examples, 218899 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:18: EPOCH 5 - PROGRESS: at 98.02% examples, 217101 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:18: EPOCH - 5 : training on 1525906 raw words (1361062 effective words) took 6.2s, 217902 effective words/s\n",
      "INFO - 18:21:18: training on a 7629530 raw words (6805820 effective words) took 30.2s, 225201 effective words/s\n",
      "/home/swetha/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n",
      "WARNING - 18:21:18: Effective 'alpha' higher than previous training cycles\n",
      "INFO - 18:21:18: training model with 2 workers on 12459 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 18:21:19: EPOCH 1 - PROGRESS: at 9.95% examples, 125313 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:20: EPOCH 1 - PROGRESS: at 25.56% examples, 165583 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:21: EPOCH 1 - PROGRESS: at 42.47% examples, 184617 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:22: EPOCH 1 - PROGRESS: at 58.59% examples, 192432 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:23: EPOCH 1 - PROGRESS: at 76.68% examples, 203264 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:24: EPOCH 1 - PROGRESS: at 96.08% examples, 211218 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:25: EPOCH - 1 : training on 1525906 raw words (1361072 effective words) took 6.4s, 212804 effective words/s\n",
      "INFO - 18:21:26: EPOCH 2 - PROGRESS: at 19.00% examples, 248367 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:27: EPOCH 2 - PROGRESS: at 37.97% examples, 249620 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:28: EPOCH 2 - PROGRESS: at 56.53% examples, 250767 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:29: EPOCH 2 - PROGRESS: at 75.48% examples, 251557 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:30: EPOCH 2 - PROGRESS: at 94.08% examples, 251090 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:30: EPOCH - 2 : training on 1525906 raw words (1361608 effective words) took 5.4s, 252015 effective words/s\n",
      "INFO - 18:21:31: EPOCH 3 - PROGRESS: at 18.31% examples, 249020 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:32: EPOCH 3 - PROGRESS: at 37.28% examples, 250631 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:33: EPOCH 3 - PROGRESS: at 55.87% examples, 251893 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:34: EPOCH 3 - PROGRESS: at 74.82% examples, 251340 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:35: EPOCH 3 - PROGRESS: at 94.08% examples, 252602 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:36: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:21:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:36: EPOCH - 3 : training on 1525906 raw words (1361196 effective words) took 5.4s, 251445 effective words/s\n",
      "INFO - 18:21:37: EPOCH 4 - PROGRESS: at 18.31% examples, 245782 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:38: EPOCH 4 - PROGRESS: at 37.28% examples, 249348 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:39: EPOCH 4 - PROGRESS: at 55.87% examples, 250684 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:40: EPOCH 4 - PROGRESS: at 74.82% examples, 251039 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:41: EPOCH 4 - PROGRESS: at 93.39% examples, 250336 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:41: EPOCH - 4 : training on 1525906 raw words (1361333 effective words) took 5.4s, 250916 effective words/s\n",
      "INFO - 18:21:42: EPOCH 5 - PROGRESS: at 19.00% examples, 251417 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:43: EPOCH 5 - PROGRESS: at 37.97% examples, 253017 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:44: EPOCH 5 - PROGRESS: at 56.53% examples, 253474 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:45: EPOCH 5 - PROGRESS: at 75.48% examples, 253864 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:46: EPOCH 5 - PROGRESS: at 94.76% examples, 253955 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:46: EPOCH - 5 : training on 1525906 raw words (1361281 effective words) took 5.4s, 253483 effective words/s\n",
      "INFO - 18:21:47: EPOCH 6 - PROGRESS: at 19.00% examples, 253949 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:48: EPOCH 6 - PROGRESS: at 37.97% examples, 255022 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:49: EPOCH 6 - PROGRESS: at 56.53% examples, 254989 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:21:50: EPOCH 6 - PROGRESS: at 71.43% examples, 241903 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:51: EPOCH 6 - PROGRESS: at 89.39% examples, 239852 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:52: EPOCH - 6 : training on 1525906 raw words (1360975 effective words) took 5.6s, 242695 effective words/s\n",
      "INFO - 18:21:53: EPOCH 7 - PROGRESS: at 18.31% examples, 247366 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:54: EPOCH 7 - PROGRESS: at 37.28% examples, 251031 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:55: EPOCH 7 - PROGRESS: at 55.87% examples, 252376 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:56: EPOCH 7 - PROGRESS: at 74.82% examples, 252406 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:57: EPOCH 7 - PROGRESS: at 94.08% examples, 253207 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:21:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:21:57: EPOCH - 7 : training on 1525906 raw words (1361224 effective words) took 5.4s, 253674 effective words/s\n",
      "INFO - 18:21:58: EPOCH 8 - PROGRESS: at 19.00% examples, 255319 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:21:59: EPOCH 8 - PROGRESS: at 37.97% examples, 255131 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:00: EPOCH 8 - PROGRESS: at 56.53% examples, 254786 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:01: EPOCH 8 - PROGRESS: at 75.48% examples, 254111 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:02: EPOCH 8 - PROGRESS: at 92.75% examples, 249362 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:03: EPOCH - 8 : training on 1525906 raw words (1361229 effective words) took 5.5s, 248645 effective words/s\n",
      "INFO - 18:22:04: EPOCH 9 - PROGRESS: at 19.00% examples, 251071 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:05: EPOCH 9 - PROGRESS: at 37.28% examples, 245231 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:06: EPOCH 9 - PROGRESS: at 55.87% examples, 248640 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:07: EPOCH 9 - PROGRESS: at 74.82% examples, 250368 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:08: EPOCH 9 - PROGRESS: at 94.08% examples, 251356 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:08: EPOCH - 9 : training on 1525906 raw words (1361174 effective words) took 5.4s, 251514 effective words/s\n",
      "INFO - 18:22:09: EPOCH 10 - PROGRESS: at 18.31% examples, 249195 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:10: EPOCH 10 - PROGRESS: at 37.28% examples, 250970 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:11: EPOCH 10 - PROGRESS: at 56.53% examples, 254218 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:12: EPOCH 10 - PROGRESS: at 75.48% examples, 254330 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:13: EPOCH 10 - PROGRESS: at 94.76% examples, 254510 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:14: EPOCH - 10 : training on 1525906 raw words (1361786 effective words) took 5.3s, 254821 effective words/s\n",
      "INFO - 18:22:15: EPOCH 11 - PROGRESS: at 19.00% examples, 251175 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:16: EPOCH 11 - PROGRESS: at 37.97% examples, 252767 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:17: EPOCH 11 - PROGRESS: at 56.53% examples, 253886 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:18: EPOCH 11 - PROGRESS: at 75.48% examples, 254409 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:19: EPOCH 11 - PROGRESS: at 94.76% examples, 254739 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:19: EPOCH - 11 : training on 1525906 raw words (1361055 effective words) took 5.3s, 254899 effective words/s\n",
      "INFO - 18:22:20: EPOCH 12 - PROGRESS: at 19.00% examples, 253239 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:21: EPOCH 12 - PROGRESS: at 37.97% examples, 252690 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:22: EPOCH 12 - PROGRESS: at 57.21% examples, 254537 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:23: EPOCH 12 - PROGRESS: at 76.16% examples, 254649 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:24: EPOCH 12 - PROGRESS: at 95.46% examples, 254085 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:24: EPOCH - 12 : training on 1525906 raw words (1361116 effective words) took 5.3s, 254786 effective words/s\n",
      "INFO - 18:22:25: EPOCH 13 - PROGRESS: at 19.00% examples, 251302 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:26: EPOCH 13 - PROGRESS: at 37.97% examples, 251548 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:27: EPOCH 13 - PROGRESS: at 56.53% examples, 251756 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:28: EPOCH 13 - PROGRESS: at 75.48% examples, 252651 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:29: EPOCH 13 - PROGRESS: at 94.76% examples, 253185 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:30: EPOCH - 13 : training on 1525906 raw words (1360949 effective words) took 5.4s, 253744 effective words/s\n",
      "INFO - 18:22:31: EPOCH 14 - PROGRESS: at 19.00% examples, 249783 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:32: EPOCH 14 - PROGRESS: at 37.97% examples, 252236 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:33: EPOCH 14 - PROGRESS: at 56.53% examples, 252284 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:34: EPOCH 14 - PROGRESS: at 75.48% examples, 253782 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:35: EPOCH 14 - PROGRESS: at 94.76% examples, 253502 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:22:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:35: EPOCH - 14 : training on 1525906 raw words (1361072 effective words) took 5.4s, 254140 effective words/s\n",
      "INFO - 18:22:36: EPOCH 15 - PROGRESS: at 19.00% examples, 253904 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:37: EPOCH 15 - PROGRESS: at 37.97% examples, 251033 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:38: EPOCH 15 - PROGRESS: at 56.53% examples, 252426 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:39: EPOCH 15 - PROGRESS: at 74.82% examples, 251139 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:40: EPOCH 15 - PROGRESS: at 94.08% examples, 252039 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:41: EPOCH - 15 : training on 1525906 raw words (1361292 effective words) took 5.4s, 250095 effective words/s\n",
      "INFO - 18:22:42: EPOCH 16 - PROGRESS: at 17.10% examples, 223608 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:43: EPOCH 16 - PROGRESS: at 36.03% examples, 236404 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:44: EPOCH 16 - PROGRESS: at 53.44% examples, 235941 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:45: EPOCH 16 - PROGRESS: at 70.77% examples, 234601 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:46: EPOCH 16 - PROGRESS: at 88.77% examples, 235213 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:46: EPOCH - 16 : training on 1525906 raw words (1361115 effective words) took 5.7s, 237541 effective words/s\n",
      "INFO - 18:22:47: EPOCH 17 - PROGRESS: at 19.00% examples, 249824 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:48: EPOCH 17 - PROGRESS: at 37.97% examples, 251515 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:49: EPOCH 17 - PROGRESS: at 57.21% examples, 253866 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:50: EPOCH 17 - PROGRESS: at 76.16% examples, 254077 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:22:51: EPOCH 17 - PROGRESS: at 95.46% examples, 253826 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:52: EPOCH - 17 : training on 1525906 raw words (1361194 effective words) took 5.3s, 254523 effective words/s\n",
      "INFO - 18:22:53: EPOCH 18 - PROGRESS: at 18.31% examples, 248191 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:54: EPOCH 18 - PROGRESS: at 37.97% examples, 252547 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:55: EPOCH 18 - PROGRESS: at 55.23% examples, 245919 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:56: EPOCH 18 - PROGRESS: at 74.20% examples, 247786 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:57: EPOCH 18 - PROGRESS: at 93.39% examples, 249366 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:22:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:22:57: EPOCH - 18 : training on 1525906 raw words (1360975 effective words) took 5.4s, 249978 effective words/s\n",
      "INFO - 18:22:58: EPOCH 19 - PROGRESS: at 19.00% examples, 250614 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:22:59: EPOCH 19 - PROGRESS: at 37.97% examples, 252010 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:00: EPOCH 19 - PROGRESS: at 56.53% examples, 252750 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:01: EPOCH 19 - PROGRESS: at 75.48% examples, 252056 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:02: EPOCH 19 - PROGRESS: at 94.76% examples, 252863 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:23:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:23:02: EPOCH - 19 : training on 1525906 raw words (1361114 effective words) took 5.4s, 253654 effective words/s\n",
      "INFO - 18:23:03: EPOCH 20 - PROGRESS: at 19.00% examples, 251309 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:23:04: EPOCH 20 - PROGRESS: at 37.97% examples, 252760 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:05: EPOCH 20 - PROGRESS: at 56.53% examples, 252818 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 18:23:07: EPOCH 20 - PROGRESS: at 75.48% examples, 253741 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:08: EPOCH 20 - PROGRESS: at 94.76% examples, 254133 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:23:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:23:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:23:08: EPOCH - 20 : training on 1525906 raw words (1361186 effective words) took 5.3s, 254511 effective words/s\n",
      "INFO - 18:23:08: training on a 30518120 raw words (27223946 effective words) took 109.5s, 248537 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 1.83 mins\n"
     ]
    }
   ],
   "source": [
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=10, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}\n",
    "\n",
    "t = time()\n",
    "\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=20, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the classics - naive bayes of the multinomial and bernoulli varieties\n",
    "# with either pure counts or tfidf features\n",
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: sentences)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: sentences)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: sentences)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: sentences)), (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: sentences)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: sentences)), (\"linear svc\", SVC(kernel=\"linear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(word2vec))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(word2vec))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "# svc_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "#                         (\"extra trees\",svc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26164286, 0.26035714, 0.25414286, 0.25835714, 0.256     ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(etree_w2v, sentences, train[1], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_w2v_tfidf.fit(sentences,train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:18:25: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('actually', 0.4714917540550232),\n",
       " ('really', 0.4584989547729492),\n",
       " ('tbh', 0.450907438993454),\n",
       " ('anyone', 0.4295053780078888),\n",
       " ('though', 0.429007887840271),\n",
       " ('dont', 0.4191727042198181),\n",
       " ('polarizing', 0.40613698959350586),\n",
       " ('lifespan', 0.39335620403289795),\n",
       " ('many_people', 0.38840723037719727),\n",
       " ('reason', 0.38497790694236755)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"honestly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.299"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"svc\", LinearSVC())])\n",
    "\n",
    "cross_val_score(lsvc, sentences, train[1], cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2976285339995668"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"svc\", LinearSVC(C=0.8))])\n",
    "\n",
    "cross_val_score(lsvc, sentences, train[1], cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2975999627750624"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"svc\", LinearSVC(C=0.8, max_iter=2500))])\n",
    "\n",
    "cross_val_score(lsvc, sentences, train[1], cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f9c1969c965e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                         (\"mnb\", MultinomialNB(alpha=0.7))])\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-73f0c55ebef6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# if a word was never seen - it must be at least as infrequent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# as any of the known words - so the default idf is the max of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1836\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1837\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1131\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mnb = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"mnb\", MultinomialNB(alpha=1.2))])\n",
    "\n",
    "cross_val_score(mnb, sentences, train[1], cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29178571428571426"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgr = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"lgr\", LogisticRegression(multi_class='multinomial',max_iter=1000))])\n",
    "\n",
    "cross_val_score(lgr, sentences, train[1], cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min count = 5, epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('australia', 0.5528700947761536),\n",
       " ('london', 0.5514118671417236),\n",
       " ('asia', 0.5451439023017883),\n",
       " ('portugal', 0.5186939239501953),\n",
       " ('france', 0.5154790878295898),\n",
       " ('moscow', 0.5140597820281982),\n",
       " ('rural', 0.503890872001648),\n",
       " ('berlin', 0.5007058382034302),\n",
       " ('apartheid', 0.5004202127456665),\n",
       " ('tokyo', 0.4965803623199463)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"paris\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('virginia', 0.592710018157959),\n",
       " ('village', 0.5891340970993042),\n",
       " ('mayor', 0.5721107721328735),\n",
       " ('1946', 0.5619644522666931),\n",
       " ('moscow', 0.5570530295372009),\n",
       " ('1976', 0.5552209615707397),\n",
       " ('paris', 0.5514118671417236),\n",
       " ('tokyo', 0.5438507199287415),\n",
       " ('istanbul', 0.539239764213562),\n",
       " ('1950s', 0.5304075479507446)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"london\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hilarious', 0.6419342756271362),\n",
       " ('amusing', 0.5643250346183777),\n",
       " ('ironic', 0.5459393262863159),\n",
       " ('annoying', 0.5240662097930908),\n",
       " ('cringey', 0.5232990384101868),\n",
       " ('silly', 0.4757176637649536),\n",
       " ('entertaining', 0.45466816425323486),\n",
       " ('dumb', 0.45443469285964966),\n",
       " ('joke', 0.4518563747406006),\n",
       " ('funnier', 0.45147237181663513)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"funny\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('happy', 0.5672705769538879),\n",
       " ('bittersweet', 0.5211690664291382),\n",
       " ('depressing', 0.510392963886261),\n",
       " ('depressed', 0.5041153430938721),\n",
       " ('nostalgic', 0.4386507272720337),\n",
       " ('uneasy', 0.4362569749355316),\n",
       " ('cringey', 0.4345662593841553),\n",
       " ('chester', 0.43363049626350403),\n",
       " ('glad', 0.43283742666244507),\n",
       " ('heartbreaking', 0.42796841263771057)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"sad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dude', 0.5487749576568604),\n",
       " ('lady', 0.4121003746986389),\n",
       " ('guy', 0.4017930030822754),\n",
       " ('holy_shit', 0.3794368505477905),\n",
       " ('girlfriend', 0.37439557909965515),\n",
       " ('granddaughter', 0.3686869144439697),\n",
       " ('15_year', 0.3599141836166382),\n",
       " ('slut', 0.35143059492111206),\n",
       " ('mother', 0.3511870503425598),\n",
       " ('ents', 0.3497922420501709)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lich', 0.680612325668335),\n",
       " ('queen', 0.5602347254753113),\n",
       " ('poro', 0.5519959330558777),\n",
       " ('robb', 0.5336294174194336),\n",
       " ('monarch', 0.5306412577629089),\n",
       " ('baratheon', 0.4936992824077606),\n",
       " ('danny', 0.489499568939209),\n",
       " ('doran', 0.48945847153663635),\n",
       " ('kneel', 0.4888492822647095),\n",
       " ('heir', 0.48488175868988037)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"king\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('valve', 0.5266107320785522),\n",
       " ('csgo', 0.4711252450942993),\n",
       " ('prescription', 0.45646488666534424),\n",
       " ('opiate', 0.4392750859260559),\n",
       " ('cheating', 0.4268868863582611),\n",
       " ('confirm', 0.41841793060302734),\n",
       " ('toxic', 0.4131224453449249),\n",
       " ('unban', 0.40976160764694214),\n",
       " ('thrower', 0.40350669622421265),\n",
       " ('folder', 0.3997250497341156)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"cheat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mincount = 10, epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:30:16: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('happy', 0.5614525079727173),\n",
       " ('depressing', 0.5262471437454224),\n",
       " ('depressed', 0.49628227949142456),\n",
       " ('shame', 0.4946313798427582),\n",
       " ('disappointed', 0.4835307002067566),\n",
       " ('chester', 0.4776112735271454),\n",
       " ('nostalgic', 0.46819111704826355),\n",
       " ('ironic', 0.4626058340072632),\n",
       " ('nervous', 0.4575343430042267),\n",
       " ('disappointing', 0.4566504657268524)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"sad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('swedish', 0.7197892069816589),\n",
       " ('finn', 0.7137970924377441),\n",
       " ('finland', 0.7105278372764587),\n",
       " ('anglo', 0.7088577747344971),\n",
       " ('estonia', 0.7073632478713989),\n",
       " ('malta', 0.69926917552948),\n",
       " ('culturally', 0.6989283561706543),\n",
       " ('heritage', 0.693010687828064),\n",
       " ('norwegian', 0.6929962635040283),\n",
       " ('authoritarian', 0.6672325134277344)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"swede\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('estonia', 0.7281621694564819),\n",
       " ('greek', 0.6562989354133606),\n",
       " ('finland', 0.6537322998046875),\n",
       " ('czech', 0.6498714089393616),\n",
       " ('bosnia', 0.6423696279525757),\n",
       " ('swede', 0.6395374536514282),\n",
       " ('republic', 0.6357571482658386),\n",
       " ('france', 0.6344835758209229),\n",
       " ('turk', 0.6317201852798462),\n",
       " ('malta', 0.6270574331283569)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"norway\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eastern_europe', 0.6872494220733643),\n",
       " ('finland', 0.6830415725708008),\n",
       " ('croatia', 0.6543526649475098),\n",
       " ('independence', 0.6518363356590271),\n",
       " ('france', 0.6482505798339844),\n",
       " ('hungary', 0.6477479934692383),\n",
       " ('turkey', 0.6448993682861328),\n",
       " ('turkish', 0.6414752006530762),\n",
       " ('ussr', 0.6368622183799744),\n",
       " ('iran', 0.6278806924819946)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"sweden\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lich', 0.7301069498062134),\n",
       " ('robb', 0.6086158752441406),\n",
       " ('queen', 0.584738552570343),\n",
       " ('aegon', 0.5654856562614441),\n",
       " ('prince', 0.5352966785430908),\n",
       " ('danny', 0.5238744020462036),\n",
       " ('monarch', 0.5233479738235474),\n",
       " ('baratheon', 0.5145795345306396),\n",
       " ('rightful', 0.51219242811203),\n",
       " ('aemon', 0.5119877457618713)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"king\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('monarch', 0.6225389242172241),\n",
       " ('king', 0.5847386121749878),\n",
       " ('heir', 0.5782822370529175),\n",
       " ('bastard', 0.5569009780883789),\n",
       " ('baratheon', 0.5562009811401367),\n",
       " ('aegon', 0.5404238700866699),\n",
       " ('elizabeth', 0.5367737412452698),\n",
       " ('wed', 0.5315330028533936),\n",
       " ('kingsguard', 0.5231883525848389),\n",
       " ('throne', 0.5116215944290161)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"queen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('genuinely', 0.498552143573761),\n",
       " ('tbh', 0.49609270691871643),\n",
       " ('actually', 0.48080170154571533),\n",
       " ('really', 0.47539612650871277),\n",
       " ('wholeheartedly', 0.4495782256126404),\n",
       " ('anyone', 0.44637858867645264),\n",
       " ('definitely', 0.43503817915916443),\n",
       " ('personally', 0.42949724197387695),\n",
       " ('realistically', 0.42518749833106995),\n",
       " ('dont', 0.40180718898773193)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"honestly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sad', 0.5614525079727173),\n",
       " ('frustrated', 0.45697569847106934),\n",
       " ('nice', 0.4280584752559662),\n",
       " ('happier', 0.4143102467060089),\n",
       " ('glad', 0.4074043929576874),\n",
       " ('wanting', 0.40049436688423157),\n",
       " ('hope', 0.40004801750183105),\n",
       " ('want', 0.3929707407951355),\n",
       " ('proud', 0.3877381384372711),\n",
       " ('upset', 0.3861738443374634)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"happy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29178571428571426"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lgr, sentences, train[1], cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iraq', 0.5791922211647034),\n",
       " ('gulf', 0.5764289498329163),\n",
       " ('afghanistan', 0.5482458472251892),\n",
       " ('casualty', 0.5424610376358032),\n",
       " ('syria', 0.5380897521972656),\n",
       " ('wwii', 0.5250870585441589),\n",
       " ('military', 0.5203325748443604),\n",
       " ('syrian', 0.5167602300643921),\n",
       " ('invading', 0.5087327361106873),\n",
       " ('naval', 0.5010626912117004)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"war\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swetha/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "AttributeError: 'list' object has no attribute 'lower'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-6612901b58d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[0;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mX_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mX_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_list_indexing\u001b[0;34m(X, key, key_dtype)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, docno)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__getitem__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocno\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Type {} does not support slicing.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentence2token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m_sentence2token\u001b[0;34m(phrase_class, sentence)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mnew_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36manalyze_sentence\u001b[0;34m(self, sentence, threshold, common_terms, scorer)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \"\"\"\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many2utf8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;31m# adding None is a trick that helps getting an automatic happy ending\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# as it won't be a common_word, nor score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/summer/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \"\"\"\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many2utf8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;31m# adding None is a trick that helps getting an automatic happy ending\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# as it won't be a common_word, nor score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(smooth_idf=False, sublinear_tf=True, max_df=0.5, use_idf=False)\n",
    "clf = ComplementNB(alpha=0.25)\n",
    "pipe = Pipeline([('vect', tfidf), ('clf', clf )])\n",
    "\n",
    "cross_val_score(pipe,sentences,train[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "summer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
